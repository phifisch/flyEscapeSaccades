{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de852206",
   "metadata": {},
   "source": [
    "# HS subtypes and spontaneous saccades\n",
    "\n",
    "Analyze Bettina's old data to extract spontaneous saccades and compare to HS subtype identity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671958eb",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e02f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathdefs import *\n",
    "#os.chdir(\"C:/Users/fischer/notebooks\")\n",
    "#os.chdir(\"C:/Users/Phil/documents/notebooks\")\n",
    "os.chdir(SOURCEPATH)\n",
    "import importlib\n",
    "#import DirectionTuning_Analyzer as dt\n",
    "#import Looming_with_background_Analyzer as lwb\n",
    "import baseClasses as bc\n",
    "#importlib.reload(bc)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from scipy import fftpack\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "from scipy import ndimage\n",
    "import pdb\n",
    "import pywt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49b91c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa35228",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class DummyAnalyzer(bc.BaseAnalyzer):\n",
    "    #LoomingWB_version = '1.0.4'#to keep track of class versions, makes easier to check for changes. Starts with 1.0.0 on 18.1.2020\n",
    "    \"\"\"version 1.0.0 changes timeax such that time coordinate 0  is at the end of looming\"\"\"\n",
    "    stimulusmap = {0.5:'loom left rot. clw', 1.0:'loom left rot. cck', 1.5:'loom left no rot.',\n",
    "                  2.0:'loom right rot. clw', 2.5:'loom right rot. cck', 3.0:'loom right no rot.',\n",
    "                  3.5:'no loom rot. clw', 4.0:'no loom rot. cck'}\n",
    "    stimulusPhases_inframes = [0,47,60,74] #frame numbers where stuff changes\n",
    "    stimulus_period_inframes = 60\n",
    "    preStim = 0.0 #s\n",
    "    afterStim = 0.0 #s\n",
    "    kineflyPeriod=0.02\n",
    "    \n",
    "    #def __init__(self, fpath, *args, **kwargs):\n",
    "    def __init__(self, loader, *args, **kwargs):\n",
    "        #loader = bc.DataLoader(fpath)\n",
    "        fpath = loader.file\n",
    "        data, samplingRate = loader.get_data()\n",
    "        samplingRate = round(samplingRate/100.,0)*100.\n",
    "        \"\"\"if data.shape[0]==5:\n",
    "            data[1,:] = signal.medfilt(data[1,:], 51)\n",
    "            data[2,:] = signal.medfilt(data[2,:], 51)\n",
    "        elif data.shape[0]==4:\n",
    "            data[1,:] = signal.medfilt(data[1,:], 9)\n",
    "            data[2,:] = signal.medfilt(data[2,:], 9)\n",
    "        \"\"\"\n",
    "        stimBound = loader.get_stimulusBoundaries()\n",
    "        stimBound[0::2] = stimBound[0::2]-self.preStim*samplingRate\n",
    "        stimBound[1::2] = stimBound[1::2]+self.afterStim*samplingRate\n",
    "        #newBounds = np.array(zip(stimBound[1::2], stimBound[0::2]))\n",
    "        #newBounds = np.concatenate(([0],stimBound,[data.shape[1],]))\n",
    "        newBounds = stimBound\n",
    "        if data[4, 0]<0.05:\n",
    "            newBounds = np.concatenate(([0], newBounds[:]))\n",
    "        if data[4, -1]<0.05:\n",
    "            newBounds = np.concatenate((newBounds[:], [data.shape[1]]))\n",
    "#         newBounds = np.concatenate(([0], stimBound[:], [data.shape[1]]))\n",
    "        #lastPiece = newBounds[-1]-newBounds[-2]\n",
    "        #lastPiece=1000\n",
    "#         lastPiece=1\n",
    "#         pdb.set_trace()\n",
    "#         print([ (newBounds[i]-newBounds[i-1],newBounds[i+1]-newBounds[i]-2*lastPiece,lastPiece) for i in range(2,newBounds.size-1,2)])\n",
    "#         data[4] = np.concatenate(( np.zeros(newBounds[1]-2*lastPiece),np.ones(lastPiece), np.zeros(lastPiece), \n",
    "#                                   np.concatenate([np.concatenate(( np.zeros(newBounds[i]-newBounds[i-1]),\n",
    "#                                                   np.zeros(newBounds[i+1]-newBounds[i]-2*lastPiece),\n",
    "#                                                   np.ones(lastPiece),np.zeros(lastPiece)))\n",
    "#                                           for i in range(2,newBounds.size-1,2)])\n",
    "#                           #, np.ones(lastPiece), np.zeros(newBounds[-1]-newBounds[-2]) #somehow too much\n",
    "#                           ))\n",
    "        metas = loader.get_metadata()\n",
    "        super(DummyAnalyzer, self).__init__(newBounds, data, Fs=samplingRate, fname=fpath.split('/')[-1], metadata=metas, **kwargs)\n",
    "        self.timeax = self.timeax-self.preStim-self.stimulusPhases_inframes[1]*self.meanFramePeriod #center 0 on end of loom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaca7d54",
   "metadata": {
    "code_folding": [
     3,
     37
    ]
   },
   "outputs": [],
   "source": [
    "# refer to https://gist.github.com/dmeliza/3251476\n",
    "# dmeliza     dmeliza/scalebars.py \n",
    "from matplotlib.offsetbox import AnchoredOffsetbox\n",
    "class AnchoredScaleBar(AnchoredOffsetbox):\n",
    "    def __init__(self, transform, sizex=0, sizey=0, labelx=None, labely=None, loc=4,\n",
    "                 pad=0.1, borderpad=0.1, sep=2, prop=None, barcolor=\"black\", barwidth=None, \n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Draw a horizontal and/or vertical  bar with the size in data coordinate\n",
    "        of the give axes. A label will be drawn underneath (center-aligned).\n",
    "        - transform : the coordinate frame (typically axes.transData)\n",
    "        - sizex,sizey : width of x,y bar, in data units. 0 to omit\n",
    "        - labelx,labely : labels for x,y bars; None to omit\n",
    "        - loc : position in containing axes\n",
    "        - pad, borderpad : padding, in fraction of the legend font size (or prop)\n",
    "        - sep : separation between labels and bars in points.\n",
    "        - **kwargs : additional arguments passed to base class constructor\n",
    "        \"\"\"\n",
    "        from matplotlib.patches import Rectangle\n",
    "        from matplotlib.offsetbox import AuxTransformBox, VPacker, HPacker, TextArea, DrawingArea\n",
    "        bars = AuxTransformBox(transform)\n",
    "        if sizex:\n",
    "            bars.add_artist(Rectangle((0,0), sizex, 0, ec=barcolor, lw=barwidth, fc=\"none\"))\n",
    "        if sizey:\n",
    "            bars.add_artist(Rectangle((0,0), 0, sizey, ec=barcolor, lw=barwidth, fc=\"none\"))\n",
    "\n",
    "        if sizex and labelx:\n",
    "            self.xlabel = TextArea(labelx, minimumdescent=False, textprops={'fontname':'arial', 'fontsize':8.})\n",
    "            bars = VPacker(children=[bars, self.xlabel], align=\"center\", pad=0, sep=sep)\n",
    "        if sizey and labely:\n",
    "            self.ylabel = TextArea(labely, textprops={'fontname':'arial', 'fontsize':8.})\n",
    "            bars = HPacker(children=[self.ylabel, bars], align=\"center\", pad=0, sep=sep)\n",
    "\n",
    "        AnchoredOffsetbox.__init__(self, loc, pad=pad, borderpad=borderpad,\n",
    "                                   child=bars, prop=prop, frameon=False, **kwargs)\n",
    "\n",
    "        \n",
    "def add_scalebar(ax, matchx=True, matchy=True, hidex=True, hidey=True, **kwargs):\n",
    "    \"\"\" Add scalebars to axes\n",
    "    Adds a set of scale bars to *ax*, matching the size to the ticks of the plot\n",
    "    and optionally hiding the x and y axes\n",
    "    - ax : the axis to attach ticks to\n",
    "    - matchx,matchy : if True, set size of scale bars to spacing between ticks\n",
    "                    if False, size should be set using sizex and sizey params\n",
    "    - hidex,hidey : if True, hide x-axis and y-axis of parent\n",
    "    - **kwargs : additional arguments passed to AnchoredScaleBars\n",
    "    Returns created scalebar object\n",
    "    \"\"\"\n",
    "    def f(axis):\n",
    "        l = axis.get_majorticklocs()\n",
    "        return len(l)>1 and (l[1] - l[0])\n",
    "    \n",
    "    if matchx:\n",
    "        kwargs['sizex'] = f(ax.xaxis)\n",
    "        kwargs['labelx'] = str(kwargs['sizex'])\n",
    "    if matchy:\n",
    "        kwargs['sizey'] = f(ax.yaxis)\n",
    "        kwargs['labely'] = str(kwargs['sizey'])\n",
    "        \n",
    "    sb = AnchoredScaleBar(ax.transData, **kwargs)\n",
    "    ax.add_artist(sb)\n",
    "\n",
    "    if hidex : ax.xaxis.set_visible(False)\n",
    "    if hidey : ax.yaxis.set_visible(False)\n",
    "    if hidex and hidey: ax.set_frame_on(False)\n",
    "\n",
    "    return sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cf35ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def subdivideSegment(segment, samplingRate, ax=None, w1_thresh=0.1, w2_thresh=0.1):\n",
    "    w1 = segment.data[1]\n",
    "    w2 = segment.data[2]\n",
    "    low1 = w1>w1_thresh\n",
    "    low2 = w2>w2_thresh\n",
    "    #medfilter is too time-consuming here, filter resulting limits instead\n",
    "    # low1 = signal.medfilt(low1,int(3*0.02*samplingRate)+1)\n",
    "    # low2 = signal.medfilt(low2,int(3*0.02*samplingRate)+1)\n",
    "    # trans = np.nonzero(np.diff(np.logical_and(low1,low2)))[0]\n",
    "    # signchange = np.diff(np.diff(np.logical_and(low1,low2)))\n",
    "    signchange = np.diff(np.logical_and(low1,low2).astype(int))\n",
    "    trans = np.nonzero(signchange)[0]\n",
    "\n",
    "    signchange = signchange[trans]\n",
    "    if len(signchange)==0:\n",
    "        #pdb.set_trace()\n",
    "        warnings.warn('Returning empty start/stop list, no rest, only \"misdetections\"')\n",
    "        #raise Exception('Invalid result: junk segment always contains some flight')\n",
    "        return [],[]\n",
    "    # distinguish where flight starts and where ends\n",
    "    starts = trans[signchange>0]\n",
    "    stops = trans[signchange<0]\n",
    "    # make start/stop list complete\n",
    "    if low1[0]:#is flying at beginning of segment\n",
    "        assert(low1[0]==low2[0])\n",
    "        starts = np.concatenate(([0], starts))\n",
    "    if low1[-1]:#flying till the end\n",
    "        assert(low1[-1]==low2[-1])\n",
    "        stops = np.append(stops, [w1.size])\n",
    "    # these are equivalent conditions, but have problem where start or stop is empty at first\n",
    "#     if stops[0]<starts[0]: # case where piece of data starts in flight\n",
    "#         assert(low1[0]==low2[0]==True)\n",
    "#         starts = np.concatenate(([0], starts))\n",
    "#     if stops[-1]<starts[-1]: # case where piece of data ends in flight\n",
    "#         assert(low1[-1]==low2[-1]==True)\n",
    "#         stops = np.append(stops, [w1.size])\n",
    "    assert(starts.size==stops.size)\n",
    "\n",
    "    # get rid of cutoffs due to measurement noise\n",
    "    #first make small misdetections disappear from start/stop list\n",
    "    rm = []\n",
    "    for i in range(len(stops)-1):\n",
    "        if starts[i+1]-stops[i]<1.5*0.02*samplingRate:\n",
    "            rm.append(i)\n",
    "    if len(rm)>0:\n",
    "        stops = np.delete(stops, rm)\n",
    "        starts = np.delete(starts, np.array(rm)+1)\n",
    "    # [ax.axvline(x, color='r', linestyle='-') for x in starts]\n",
    "    # [ax.axvline(x, color='r', linestyle=':') for x in stops]\n",
    "\n",
    "    # #give some extra slack\n",
    "    starts += int(0.15*samplingRate) #150ms\n",
    "    stops -= int(0.15*samplingRate)\n",
    "\n",
    "    # #check if flight bouts are too short, only flight, don't care about rest periods here anyway\n",
    "    rm = []\n",
    "    for i in range(len(starts)):\n",
    "        if stops[i]-starts[i] < 2.1*0.02*samplingRate:\n",
    "            rm.append(i)\n",
    "    if len(rm)>0:\n",
    "        starts = np.delete(starts, rm)\n",
    "        stops = np.delete(stops, rm)\n",
    "    assert(starts.size==stops.size)\n",
    "    # [ax.axvline(x, color='g', linestyle='-') for x in starts]\n",
    "    # [ax.axvline(x, color='g', linestyle=':') for x in stops]\n",
    "    return starts,stops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50bf53d",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436397d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.join(DATAPATH,'vonBettina','oldHSdata_adaptedFormat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a88c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileGen(root):\n",
    "    dircontent = os.listdir(root) \n",
    "    for f in dircontent:\n",
    "        print(f)\n",
    "        yield f\n",
    "\n",
    "fileIterator = fileGen(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79874f9d",
   "metadata": {},
   "source": [
    "## sanity check for datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53c2b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(root)\n",
    "fn = os.listdir(root)[22]\n",
    "fn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52f1060b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# test if all files work ok now\n",
    "dircontent = os.listdir(root) \n",
    "for fl in dircontent[:]:\n",
    "    if not '2013' in fl: continue\n",
    "    print(fl)\n",
    "    loader = bc.DataLoader(os.path.join(root,fl))\n",
    "    anlyz = DummyAnalyzer(loader, meanFramePeriod=0.062)\n",
    "    f,ax=plt.subplots(1,1)\n",
    "    [ax.plot(s.data[anlyz.channelmap['arenaOut'],::10]) for s in anlyz.segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69450e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,1)\n",
    "ax.plot(d)\n",
    "ax.plot(loader.data[-2])\n",
    "# [ax.axvline(x, color='k') for x in b]\n",
    "[ax.axvline(x, color='r') for x in b[:-1][np.diff(b)<10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23532d00",
   "metadata": {},
   "source": [
    "I removed the file from 2012 02 14 cell2 because it has a very different format, like a different order of channels, experiment procedure or something. I don't know, have to ask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef2fddb",
   "metadata": {},
   "source": [
    "## get file of fly"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0f19765",
   "metadata": {},
   "source": [
    "fl=next(fileIterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3422f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell should be repeated after initiating above\n",
    "#fn = next(fileIterator)\n",
    "print(fn)\n",
    "# if not '2013' in fn:\n",
    "#     assert False\n",
    "loader = bc.DataLoader(os.path.join(root,fn))\n",
    "anlyz = DummyAnalyzer(loader, meanFramePeriod=0.05)\n",
    "for k in list(anlyz.metadata[0].keys()):\n",
    "    if k[:2]=='HS':\n",
    "        anlyz.metadata[0]['HS_type']=k\n",
    "f,ax=plt.subplots(1,1)\n",
    "[ax.plot(s.data[anlyz.channelmap['arenaOut'],::10]) for s in anlyz.segments]\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97af33c",
   "metadata": {},
   "source": [
    "## quality control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb03d605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for problems with Vm during non-stimulus periods\n",
    "f,ax=plt.subplots(1,1)\n",
    "reductionFactor = 10\n",
    "timeax = range(0,loader.data.shape[1], reductionFactor)\n",
    "ax.plot(timeax, loader.data[0,::reductionFactor])\n",
    "ax.plot(timeax, loader.data[1,::reductionFactor])\n",
    "ax.plot(timeax, loader.data[2,::reductionFactor])\n",
    "ax.plot(timeax, loader.data[4,::reductionFactor]*10.)# make more prominent\n",
    "sb =loader.get_stimulusBoundaries()\n",
    "for i in range(0,len(sb),2):\n",
    "    ax.axvline(sb[i], linestyle='--', color='k')\n",
    "    ax.axvline(sb[i+1], linestyle=':', color='k')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00069ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm=[]\n",
    "for i,s in enumerate(anlyz.segments):\n",
    "    if s.boundaries[0]>1.075e7 and s.boundaries[0]<1.255e7:\n",
    "    #if s.boundaries[0]>4e6:\n",
    "        print(s.boundaries)\n",
    "        rm.append(i)\n",
    "print(rm)\n",
    "\n",
    "if input(\"is this ok? [y/n]  \").lower()=='y':\n",
    "    anlyz.remove_duds_byList(rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd690a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how long are trials\n",
    "f,ax=plt.subplots(1,1)\n",
    "ax.hist([(s.boundaries[1]-s.boundaries[0])/anlyz.samplingRate for s in anlyz.segments ], \n",
    "        bins=np.concatenate((np.arange(0,5,1), np.arange(5,20,2),np.arange(20,200,10))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dc8918",
   "metadata": {},
   "source": [
    "## how much flying?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b95558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#anlyz.plot_by_stimSignal('wingR')\n",
    "f,ax=plt.subplots(1,1, figsize=(9,6))\n",
    "ax.plot(np.arange(0,loader.data.shape[1],5)/anlyz.samplingRate,loader.data[1,::5])\n",
    "ax.plot(np.arange(0,loader.data.shape[1],5)/anlyz.samplingRate, loader.data[2,::5])\n",
    "ax.plot(np.arange(0,loader.data.shape[1],5)/anlyz.samplingRate, loader.data[-1,::5])\n",
    "# ax.plot(np.arange(0,loader.data.shape[1],5)/anlyz.samplingRate, loader.data[3,::5])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa4a1c25",
   "metadata": {},
   "source": [
    "f,ax=plt.subplots(1,1)\n",
    "[ ax.plot(range(*s.boundaries),s.data[1], 'b-') for s in anlyz.segments ]\n",
    "[ ax.plot(range(*s.boundaries),s.data[2], 'r-') for s in anlyz.segments ]\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f252ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,1)\n",
    "#[ax.hist(s.data[1],bins=np.arange(0,3.8,0.1), alpha=0.3) for s in anlyz.segments[:5] ]\n",
    "ax.hist(loader.data[1],bins=np.arange(0,3.8,0.05), alpha=0.4)\n",
    "ax.hist(loader.data[2],bins=np.arange(0,3.8,0.05), alpha=0.4)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f605ba98",
   "metadata": {},
   "source": [
    "# Preselect flying trials\n",
    "\n",
    "That means, the intertrial intervals where the fly was flying. Do it like usual in the other notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3954d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VERY SIMPLE METHOD, COUNTING THRESHOLDED VALUES AND THRESHOLD THAT\n",
    "theta_1 = 1.9\n",
    "theta_2 = 1.6\n",
    "f,ax=plt.subplots(1,1)\n",
    "#t2=np.array([np.sum(np.logical_or(anlyz.segments[i].data[2]>1.6, np.logical_and(anlyz.segments[i].data[2]>-theta, anlyz.segments[i].data[2]<theta))) for i in range(anlyz.segments.size)])\n",
    "\n",
    "# t1= np.array([np.sum(np.logical_or(anlyz.segments[i].data[1] < 1.6 ,\n",
    "#                                    anlyz.segments[i].data[1]<theta) ) for i in range(anlyz.segments.size)])\n",
    "# t1= np.array([np.sum(np.logical_or(anlyz.segments[i].data[1] < 1.6 ,\n",
    "#                                    anlyz.segments[i].data[1]<theta) ) for i in range(anlyz.segments.size)])\n",
    "t1= np.array([np.sum(anlyz.segments[i].data[1]<theta_1) for i in range(anlyz.segments.size)])\n",
    "# t2= np.array([np.sum(np.logical_or(anlyz.segments[i].data[2] > 3.35 ,\n",
    "#                                    anlyz.segments[i].data[2]<theta_2) ) for i in range(anlyz.segments.size)])\n",
    "t2= np.array([np.sum(anlyz.segments[i].data[2] < theta_2) for i in range(anlyz.segments.size)])\n",
    "\n",
    "# ax.hist(t1,alpha=0.5, bins=np.arange(-250,t1.max()+250, 500))\n",
    "# ax.hist(t2,alpha=0.5, bins=np.arange(-250,t2.max()+250, 500))\n",
    "segmentLengths = np.array([s.data.shape[1] for s in anlyz.segments])\n",
    "ax.hist(t1/segmentLengths,alpha=0.5, bins=np.arange(0,1,0.025))\n",
    "ax.hist(t2/segmentLengths,alpha=0.5, bins=np.arange(0,1,0.025))\n",
    "\n",
    "f,ax=plt.subplots(1,1)\n",
    "# jitterForPlotting = np.random.rand(len(t1))/2\n",
    "# ax.plot(t1/segmentLengths+jitterForPlotting,  t2/segmentLengths,'k.')\n",
    "ax.plot(t1/segmentLengths,  t2/segmentLengths,'k.')\n",
    "ax.set_xlabel('prop. values under threshold for wing L')\n",
    "ax.set_ylabel('prop. values under threshold for wing R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be49104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLORIZE data points (trials) that are (almost) all below threshold (nf=non-flying)\n",
    "segmentLengths = np.array([s.data.shape[1] for s in anlyz.segments])\n",
    "discrepancyTol = [max(sl*0.02, 0.05*anlyz.samplingRate) for sl in segmentLengths]\n",
    "nf = np.where(np.logical_and(np.abs(t1-t2)<discrepancyTol, t1>(segmentLengths*0.9) ))[0]\n",
    "#ax.plot(t1[nf]+jitterForPlotting[nf], t2[nf],'r.')\n",
    "ax.plot(t1[nf]/segmentLengths[nf], t2[nf]/segmentLengths[nf],'r.')\n",
    "nf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41a9aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLORIZE data points (trials) that are all above threshold (fl=flying)\n",
    "fl = np.where(np.logical_and(np.abs(t1-t2)<0.03*anlyz.samplingRate, t1<0.03*anlyz.samplingRate))[0]\n",
    "#fl = np.where(np.logical_and(np.abs(t1-t2)<0.11*anlyz.samplingRate, t1<0.05*anlyz.samplingRate))[0]\n",
    "# ax.plot(t1[fl]+jitterForPlotting[fl], t2[fl],'b.')\n",
    "ax.plot(t1[fl]/segmentLengths[fl], t2[fl]/segmentLengths[fl],'b.')\n",
    "fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define junk trials (=jk) as remaining ones, and colorize in plot\n",
    "jk = np.ones(anlyz.segments.size, dtype=bool)\n",
    "jk[nf]=False\n",
    "jk[fl]=False\n",
    "jk = np.nonzero(jk)[0]\n",
    "# p=plt.plot(t1[jk]+jitterForPlotting[jk], t2[jk],'y.')\n",
    "p=plt.plot(t1[jk]/segmentLengths[jk], t2[jk]/segmentLengths[jk],'y.')\n",
    "jk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3cec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fl.size)\n",
    "print(nf.size)\n",
    "print(jk.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a41fc",
   "metadata": {},
   "source": [
    "### intervene in \"junk\" trials to rescue\n",
    "#### figure out which parts to keep and store as new BaseAnalyzer instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b7e398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for trialNo in jk:\n",
    "    #print(f'{trialNo=}')#new python version\n",
    "    print(f'trialNo={trialNo}')\n",
    "    orig_boundaries = anlyz.segments[trialNo].boundaries\n",
    "    starts,stops = subdivideSegment(anlyz.segments[trialNo], anlyz.samplingRate)\n",
    "    if len(starts)==0:\n",
    "        continue\n",
    "    newAnlyz = bc.BaseAnalyzer(np.sort(np.concatenate((starts,stops))), anlyz.segments[trialNo].data[:-1] ,\n",
    "                Fs=anlyz.samplingRate, metadata=anlyz.metadata[0], meanFramePeriod=anlyz.meanFramePeriod)\n",
    "    # get rid of segments that are too short\n",
    "    print(f'#segments before cleaning: {len(newAnlyz)}')\n",
    "    rm=[]\n",
    "    for i in range(len(newAnlyz)):\n",
    "        if len(newAnlyz.segments[i]) < 1.5*anlyz.samplingRate:\n",
    "            rm.append(i)\n",
    "    newAnlyz.remove_duds_byList(rm)\n",
    "    print(f'#segments after cleaning: {len(newAnlyz)}')\n",
    "    # correct segment boundaries for original offset, in case we need them at some point\n",
    "    for seg in newAnlyz.segments:\n",
    "        seg.boundaries += orig_boundaries[0]\n",
    "    if not len(newAnlyz)==0:\n",
    "        res.append(newAnlyz)\n",
    "newAnlyz = sum(res)\n",
    "print(f'#segments rescued total: {len(newAnlyz)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64eb7a5",
   "metadata": {},
   "source": [
    "#### feed new results into original analyzer\n",
    "need to delete the junky trials first, then add our new analyzer to old one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e1b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(anlyz), len(jk))\n",
    "anlyz.remove_duds_byList(jk)\n",
    "print(len(anlyz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a0430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anlyz = anlyz + newAnlyz\n",
    "print(len(anlyz))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb827460",
   "metadata": {},
   "source": [
    "#### redo the calculation done above before converging to main branch\n",
    "We need to establish that all our newly spawned segments adhere to the same standards and are not \"junky\" by themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ab73be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VERY SIMPLE METHOD, COUNTING THRESHOLDED VALUES AND THRESHOLD THAT\n",
    "print(f'parameters for deciding if wing is beating and detected properly\\n theta wing 1: {theta_1}\\ntheta wing 2: {theta_2}')\n",
    "# f,ax=plt.subplots(1,1)\n",
    "t1= np.array([np.sum(anlyz.segments[i].data[1]<theta_1) for i in range(anlyz.segments.size)])\n",
    "t2= np.array([np.sum(anlyz.segments[i].data[2] < theta_2) for i in range(anlyz.segments.size)])\n",
    "\n",
    "segmentLengths = np.array([s.data.shape[1] for s in anlyz.segments])\n",
    "# ax.hist(t1/segmentLengths,alpha=0.5, bins=np.arange(0,1,0.025))\n",
    "# ax.hist(t2/segmentLengths,alpha=0.5, bins=np.arange(0,1,0.025))\n",
    "# f,ax=plt.subplots(1,1)\n",
    "# ax.plot(t1/segmentLengths,  t2/segmentLengths,'k.')\n",
    "# ax.set_xlabel('prop. values under threshold for wing L')\n",
    "# ax.set_ylabel('prop. values under threshold for wing R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23dca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLORIZE data points (trials) that are (almost) all below threshold (nf=non-flying)\n",
    "segmentLengths = np.array([s.data.shape[1] for s in anlyz.segments])\n",
    "discrepancyTol = [max(sl*0.02, 0.05*anlyz.samplingRate) for sl in segmentLengths]\n",
    "nf = np.where(np.logical_and(np.abs(t1-t2)<discrepancyTol, t1>(segmentLengths*0.9) ))[0]\n",
    "# ax.plot(t1[nf]/segmentLengths[nf], t2[nf]/segmentLengths[nf],'r.')\n",
    "print(f'nf: {nf}')\n",
    "\n",
    "# COLORIZE data points (trials) that are all above threshold (fl=flying)\n",
    "fl = np.where(np.logical_and(np.abs(t1-t2)<0.03*anlyz.samplingRate, t1<0.03*anlyz.samplingRate))[0]\n",
    "#fl = np.where(np.logical_and(np.abs(t1-t2)<0.11*anlyz.samplingRate, t1<0.05*anlyz.samplingRate))[0]\n",
    "# ax.plot(t1[fl]/segmentLengths[fl], t2[fl]/segmentLengths[fl],'b.')\n",
    "print(f'fl: {fl}')\n",
    "\n",
    "# define junk trials (=jk) as remaining ones, and colorize in plot\n",
    "jk = np.ones(anlyz.segments.size, dtype=bool)\n",
    "jk[nf]=False\n",
    "jk[fl]=False\n",
    "jk = np.nonzero(jk)[0]\n",
    "# p=ax.plot(t1[jk]/segmentLengths[jk], t2[jk]/segmentLengths[jk],'y.')\n",
    "print(f'jk: {jk}')\n",
    "\n",
    "print(f'#flying: {fl.size}')\n",
    "print(f'#non-flying: {nf.size}')\n",
    "print(f'#junky/uncertain: {jk.size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eebf4df",
   "metadata": {},
   "source": [
    "### continue with deletion and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07f0404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#anlyz.remove_duds_byList(jk)\n",
    "nonflying = nf\n",
    "flying = fl\n",
    "toRemove=jk\n",
    "anlyz.flying[nonflying]=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d8ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [j-np.sum(anlyz.flying[:j]==False) for j in toRemove]\n",
    "\n",
    "## order of operations is important here\n",
    "anlyz.remove_duds_byList(nonflying)\n",
    "anlyz.remove_duds_byList(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63514552",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## VERY SIMPLE METHOD, COUNTING THRESHOLDED VALUES AND THRESHOLD THAT\n",
    "theta = 0.1\n",
    "f,ax=plt.subplots(1,2)\n",
    "\n",
    "print(f'parameters for deciding if wing is beating and detected properly\\n theta wing 1: {theta_1}\\ntheta wing 2: {theta_2}')\n",
    "# f,ax=plt.subplots(1,1)\n",
    "t1= np.array([np.sum(anlyz.segments[i].data[1]<theta_1) for i in range(anlyz.segments.size)])\n",
    "t2= np.array([np.sum(anlyz.segments[i].data[2] < theta_2) for i in range(anlyz.segments.size)])\n",
    "segmentLengths = np.array([s.data.shape[1] for s in anlyz.segments])\n",
    "\n",
    "ax[0].hist(t1/segmentLengths,alpha=0.5, bins=np.arange(0,1,0.025))\n",
    "ax[0].hist(t2/segmentLengths,alpha=0.5, bins=np.arange(0,1,0.025))\n",
    "\n",
    "# jitterForPlotting = np.random.rand(len(t1))/2\n",
    "# ax.plot(t1/segmentLengths+jitterForPlotting,  t2/segmentLengths,'k.')\n",
    "ax[1].plot(t1/segmentLengths,  t2/segmentLengths,'k.')\n",
    "ax[1].set_xlabel('prop. values under threshold for wing L')\n",
    "ax[1].set_ylabel('prop. values under threshold for wing R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ad11b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,1, figsize=(9,6))\n",
    "[ ax.plot(range(*s.boundaries),s.data[1], 'b-') for s in anlyz.segments ]\n",
    "[ ax.plot(range(*s.boundaries),s.data[2], 'r-') for s in anlyz.segments ]\n",
    "#[ ax.plot(range(*s.boundaries),s.data[0], 'g-') for s in anlyz.segments ]\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "257e0496",
   "metadata": {},
   "source": [
    "rm=[]\n",
    "for i,s in enumerate(anlyz.segments):\n",
    "    if s.boundaries[1]<0.9e6:\n",
    "        print(s.boundaries)\n",
    "        rm.append(i)\n",
    "print(rm)\n",
    "\n",
    "if input(\"is this ok? [y/n]  \").lower()=='y':\n",
    "    anlyz.remove_duds_byList(rm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996c8415",
   "metadata": {},
   "source": [
    "# Detect Saccades by CWT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8737bf",
   "metadata": {},
   "source": [
    "## begin by fitting the PWC wingdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d5b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helperLibrary\n",
    "\n",
    "res={}\n",
    "f,ax=plt.subplots(1,1)\n",
    "for i,seg in enumerate(anlyz.segments):\n",
    "    yfit, xsteps, ysteps = helperLibrary.straightLineFiltering(seg.data[anlyz.channelmap['wingDiff']], 0.020*anlyz.samplingRate, postEnhancement=True)\n",
    "    res[i] = ysteps\n",
    "    ax.plot(np.diff(xsteps))\n",
    "ax.set_title('diff of xsteps: Values very different from 200 should only occur at start or end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7e6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pureYvals = res"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b21b668",
   "metadata": {},
   "source": [
    "## check if resulting PWC also holds for this data\n",
    "## --> YES IT DOES\n",
    "segId = 16\n",
    "yfit, xsteps, ysteps = helperLibrary.straightLineFiltering(anlyz.segments[segId].data[anlyz.channelmap['wingDiff']],\n",
    "                                    0.020*anlyz.samplingRate, postEnhancement=True )\n",
    "f,ax=plt.subplots(1,1)\n",
    "ax.plot(anlyz.segments[segId].data[-1])\n",
    "ax.plot(yfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb4bfac",
   "metadata": {},
   "source": [
    "## function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05e945c",
   "metadata": {
    "code_folding": [
     8,
     15,
     21,
     30,
     51,
     56,
     64,
     87,
     109,
     133,
     144,
     154
    ]
   },
   "outputs": [],
   "source": [
    "def lowPass(y, samplingRate):\n",
    "    fThresh = 6.#Hz\n",
    "    #b,a=signal.butter(5,6.,fs=samplingRate, btype='low')\n",
    "    #ylow = signal.lfilter(b,a,y)\n",
    "    b,a=signal.butter(5,6.*1.5,fs=samplingRate, btype='low')\n",
    "    ylow = signal.filtfilt(b,a,y)\n",
    "    return ylow\n",
    "\n",
    "def central_diff_derivative(y, halfwidth=0.070, samplPeriod=0.020):\n",
    "    \"\"\"y is data to derivate, assumed to be np.array\n",
    "    halfwidth and samplPeriod shall use the same unit (e.g. seconds)\"\"\"\n",
    "    stepBack = int(halfwidth/samplPeriod+0.5)\n",
    "    stepFwd = int(halfwidth/samplPeriod)\n",
    "    return ( y[(stepFwd+stepBack):]-y[:-(stepBack+stepFwd)] )/((stepFwd+stepBack)*samplPeriod)\n",
    "\n",
    "def highpass_data(y, samplingRate):\n",
    "    # highpass data\n",
    "    b,a = signal.butter(5, 1., fs=samplingRate, btype='high')\n",
    "    yhigh = signal.lfilter(b,a,y)\n",
    "    return yhigh\n",
    "\n",
    "def plotHighLowPass(y, yhigh, ylow=None):\n",
    "    f,ax=plt.subplots(1,1)\n",
    "    ax.plot(y)\n",
    "    ax.plot(yhigh)\n",
    "    if not ylow is None:\n",
    "        ax.plot(ylow)\n",
    "    ax.legend(['original','highpass', 'lowpass'])\n",
    "    return ax\n",
    "\n",
    "def extractFlyStatistics(y, samplingRate, plot=True, **kwargs):\n",
    "    yhigh = highpass_data(y, samplingRate)\n",
    "    pdf, bins = np.histogram(yhigh, bins=50)\n",
    "    pdf = pdf/pdf.sum()\n",
    "    cdf = np.cumsum(pdf)\n",
    "    thresh = kwargs.get('thresh',0.95)\n",
    "    binCenters = (bins[:-1]+bins[1:])/2\n",
    "\n",
    "    minIdx = np.argmin(np.abs(cdf-thresh))\n",
    "    theta = binCenters[minIdx]\n",
    "    \n",
    "    if plot:\n",
    "        f,ax=plt.subplots(1,1)\n",
    "        ax.plot(binCenters, pdf)\n",
    "        ax.plot(binCenters, cdf)\n",
    "        ax.plot(binCenters[minIdx],cdf[minIdx], 'go')\n",
    "        ax.plot(binCenters[minIdx],pdf[minIdx], 'ro')\n",
    "        ax.axhline(0.9, color='k', linestyle=':')\n",
    "        ax.axvline(theta, color='r')\n",
    "    return theta\n",
    "\n",
    "def detectPeaks(theta, ydiff):\n",
    "    detectTheta = theta*6. #as stated in paper\n",
    "    peaksFound = signal.find_peaks(np.abs(ydiff), height=detectTheta)\n",
    "    return peaksFound\n",
    "\n",
    "def plotDetection(ax,ydiff, detectTheta):\n",
    "    #ax=plt.gca()\n",
    "    #ax.plot(np.abs(ydiff))\n",
    "    ax.plot(ydiff)\n",
    "    ax.axhline(detectTheta, color='k')    \n",
    "    ax.axhline(0, color='g')\n",
    "    \n",
    "# find peak times\n",
    "def findPeakTimes(ydiff, peaksFound):\n",
    "    ## for each peak, move FORWARD till encountering closest to 0\n",
    "    peakTime = np.full_like(peaksFound, np.nan, dtype=int)\n",
    "    for j,p in enumerate(peaksFound):\n",
    "        #compare the signs, because peaks can be + or - (at 0 it switches)\n",
    "        sameSign = True\n",
    "        i=0\n",
    "        while sameSign>0:\n",
    "            i+=1\n",
    "            if p+i >=ydiff.size:\n",
    "                i -=1 #plug right value for line after while loop\n",
    "                break\n",
    "            sameSign = (ydiff[p+i]*ydiff[p+i-1])>0. \n",
    "        if p+i <ydiff.size:\n",
    "            thisPeak = p+i-1 + np.argmin(np.abs(ydiff[[p+i-1,p+i]]))\n",
    "        else:\n",
    "            thisPeak = ydiff.size\n",
    "        #print(ydiff[[p+i-1,p+i]])\n",
    "        #print(thisPeak)\n",
    "        peakTime[j] = int(thisPeak)\n",
    "    return peakTime\n",
    "\n",
    "# find onset times\n",
    "def findOnsetTime(ydiff, peaksFound):\n",
    "    ## for each peak, move BACKWARD till encountering closest to 0\n",
    "    onsetTime = np.zeros_like(peaksFound, dtype=int)\n",
    "    for j,p in enumerate(peaksFound):\n",
    "        sameSign = True\n",
    "        i=-0\n",
    "        while sameSign>0:\n",
    "            i-=1\n",
    "            if p+i<0:\n",
    "                i+=1 #plug right value for line after while loop\n",
    "                break\n",
    "            sameSign = (ydiff[p+i]*ydiff[p+i+1])>0. #compare the signs, because peaks can be + or - (at 0 it switches)\n",
    "        if p+i >=0:\n",
    "            thisOnset = p+i + np.argmin(np.abs(ydiff[[p+i,p+i+1]]))\n",
    "        else:\n",
    "            thisOnset=0\n",
    "        #print(ydiff[[p+i,p+i+1]])\n",
    "        #print(thisOnset)\n",
    "        onsetTime[j] = int(thisOnset)\n",
    "    return onsetTime\n",
    "\n",
    "# correct onset times for error introduced by lowpass filtering\n",
    "def correctOnsetTime(y,ylow, ydiff, peaksFound, onsetTime):\n",
    "    actualOnset = np.full_like(onsetTime, np.nan, dtype=int)\n",
    "    for j,o in enumerate(onsetTime):\n",
    "        ## use value of low-passed L-R WBA at putative onset as threshold\n",
    "        thresh = ylow[o]\n",
    "        ## look forward in original signal from putative onset and find last sample before threshold\n",
    "        ytemp = y-thresh #this turns it into finding a 0-crossing (like above)\n",
    "        ### shortcut to deal with cases where actual onset is not included in trace\n",
    "        p = peaksFound[j]\n",
    "        if not any(np.diff(np.sign(ytemp[o:p]))):\n",
    "            #no change in sign means no threshold crossing\n",
    "            actualOnset[j] = o #keep putataive one\n",
    "            continue\n",
    "        sameSign=True\n",
    "        i=0\n",
    "        while sameSign:\n",
    "            i+=1\n",
    "            #sameSign = (ydiff[o+i]*ydiff[o+i-1])>0.\n",
    "            sameSign = (ytemp[o+i]*ytemp[o+i-1])>0.\n",
    "        ## define this as actual onset\n",
    "        actualOnset[j] = o+1-1\n",
    "    return actualOnset\n",
    "\n",
    "## get saccade amplitudes\n",
    "def getSaccadeAmplitude(y, peaksFound, actualOnset, samplingRate):\n",
    "    #subtracted the mean L-R WBA signal 50-ms baseline interval before saccade onset \n",
    "    #from mean L-R WBA signal in +-15-ms windows surrounding the peak\n",
    "    saccAmp = np.full(peaksFound.size, np.nan, dtype=np.float)\n",
    "    for i in range(len(actualOnset)):\n",
    "        baseline = np.mean(y[actualOnset[i]-int(0.06*samplingRate) : actualOnset[i]])\n",
    "        if not np.isnan(baseline):\n",
    "            #saccAmp[i] =  np.mean( y[peaksFound[i]-int(0.03*samplingRate) : peaksFound[i]+int(0.03*samplingRate)+1] ) - baseline    \n",
    "            saccAmp[i] =  np.mean( y[peaksFound[i]-int(0.03*samplingRate) : peaksFound[i]+int(0.03*samplingRate)] ) - baseline    \n",
    "    return saccAmp\n",
    "\n",
    "def plotPeaks(y, actualOnset, peakTime, saccAmp, ax=None):\n",
    "    if ax is None:\n",
    "        f,ax=plt.subplots(1,1)\n",
    "        ax.plot(y)\n",
    "    ax.plot(actualOnset, y[actualOnset], 'r>')\n",
    "    ax.plot(peakTime, y[peakTime], 'k^')\n",
    "    ax.plot(peakTime, saccAmp, 'ko')\n",
    "    return\n",
    "\n",
    "# eliminate putative saccade if maximal deviation before onset is>55% of amplitude\n",
    "def eliminateByDeviation(y, actualOnset, saccAmp, samplingRate, **kwargs):\n",
    "    window = kwargs.get('window',0.201)\n",
    "    deviationLimit = kwargs.get('deviationLimit',0.55)\n",
    "    toEliminate = []\n",
    "    for i in range(actualOnset.size):\n",
    "        # preselect to make sure the indexing is feasible\n",
    "        #baseline = np.mean(y[actualOnset[i]-int(0.2*samplingRate) : actualOnset[i]])\n",
    "        chunk = y[actualOnset[i]-int(window*samplingRate) : actualOnset[i]]\n",
    "        if actualOnset[i]-int(window*samplingRate) <0:\n",
    "            toEliminate.append(i)\n",
    "            continue\n",
    "        deviation = chunk.max()-chunk.min()\n",
    "        if deviation>deviationLimit*abs(saccAmp[i]):\n",
    "            toEliminate.append(i)\n",
    "    return toEliminate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816c2436",
   "metadata": {
    "code_folding": [
     0,
     19,
     41,
     65,
     89,
     96
    ]
   },
   "outputs": [],
   "source": [
    "def eliminateByPeakProximity(onset, peakTime,*, samplingRate=10000, timeDist:float=0.5) ->list:\n",
    "    '''eliminate peaks that are too close too one another. \n",
    "    This is intended particularly for when they go in opposite direction, \n",
    "    such that the second could well be end of the first\n",
    "    Note: we don't need absolute times because operating inside segments\n",
    "    This is instance one, measuring distance peak-to-onset'''\n",
    "    #make sure they are sorted\n",
    "    onset.sort()\n",
    "    peakTime.sort()\n",
    "    # iterate forward, check distance to previous \n",
    "    toEliminate = []\n",
    "    for i in range(1,onset.size):\n",
    "        current=onset[i]\n",
    "        prev = peakTime[i-1]\n",
    "        # if it's too short, eliminate\n",
    "        if current-prev < timeDist*samplingRate :\n",
    "            toEliminate.append(i)\n",
    "    return toEliminate\n",
    "\n",
    "def eliminateByPeakProximity2(onset,*, samplingRate=10000, timeDist:float=0.5) ->list:\n",
    "    '''eliminate peaks that are too close to one another. \n",
    "    This is intended particularly for when they go in opposite direction, \n",
    "    such that the second could well be end of the first\n",
    "    Note: we don't need absolute times because operating inside segments\n",
    "    This is instance two, measuring distance onset-to-onset'''\n",
    "    #make sure they are sorted\n",
    "    onset.sort()\n",
    "    peakTime.sort()\n",
    "    # iterate forward, check distance to previous \n",
    "    toEliminate = []\n",
    "    for i in range(1,onset.size):\n",
    "        current=onset[i]\n",
    "        prev = onset[i-1]\n",
    "        # if it's too short, eliminate\n",
    "#         if not (current-prev)>0:\n",
    "#             print('weird stuff here ',current, prev)\n",
    "        assert(not current-prev<0)\n",
    "        if current-prev < timeDist*samplingRate :\n",
    "            toEliminate.append(i)\n",
    "    return toEliminate\n",
    "\n",
    "def eliminateByPeakProximity3(onset,saccAmp,*, samplingRate=10000, timeDist:float=0.5) ->list:\n",
    "    '''eliminate peaks that are too close to one another. \n",
    "    This is intended particularly for when they go in opposite direction, \n",
    "    such that the second could well be end of the first\n",
    "    Note: we don't need absolute times because operating inside segments\n",
    "    This is instance three, considering only distances between saccades if they are in opposite directions'''\n",
    "    #make sure they are sorted\n",
    "    onset.sort()\n",
    "    peakTime.sort()\n",
    "    # iterate forward, check distance to previous \n",
    "    toEliminate = []\n",
    "    for i in range(1,onset.size):\n",
    "        if saccAmp[i]*saccAmp[i-1] >0:#sign check, same saccade direction\n",
    "            continue\n",
    "        current=onset[i]\n",
    "        prev = onset[i-1]\n",
    "        # if it's too short, eliminate\n",
    "#         if not (current-prev)>0:\n",
    "#             print('weird stuff here ',current, prev)\n",
    "        assert(not current-prev<0)\n",
    "        if current-prev < timeDist*samplingRate :\n",
    "            toEliminate.append(i)\n",
    "    return toEliminate\n",
    "\n",
    "def eliminateByPeakProximity4(onset,peakTime,saccAmp,*, samplingRate=10000, timeDist:float=0.5) ->list:\n",
    "    '''eliminate peaks that are too close to one another. \n",
    "    This is intended particularly for when they go in opposite direction, \n",
    "    such that the second could well be end of the first\n",
    "    Note: we don't need absolute times because operating inside segments\n",
    "    This is instance three, considering only distances between saccades if they are in opposite directions'''\n",
    "    #make sure they are sorted\n",
    "    onset.sort()\n",
    "    peakTime.sort()\n",
    "    # iterate forward, check distance to previous \n",
    "    toEliminate = []\n",
    "    for i in range(1,onset.size):\n",
    "        if saccAmp[i]*saccAmp[i-1] >0:#sign check, same saccade direction\n",
    "            continue\n",
    "        current=onset[i]\n",
    "        prev = peakTime[i-1]\n",
    "        # if it's too short, eliminate\n",
    "#         if not (current-prev)>0:\n",
    "#             print('weird stuff here ',current, prev)\n",
    "#         assert(not current-prev<0)#problkem with this assertion\n",
    "        if current-prev < timeDist*samplingRate :\n",
    "            toEliminate.append(i)\n",
    "    return toEliminate\n",
    "\n",
    "def eliminateBySaccAmplitude(saccAmp, thresh)->list:\n",
    "    toEliminate=[]\n",
    "    for i in range(0,saccAmp.size):\n",
    "        if abs(saccAmp[i])< thresh:\n",
    "            toEliminate.append(i)\n",
    "    return toEliminate\n",
    "\n",
    "def eliminatePutativePersistent(cwtpeaks,peakTime,maxPeakDelay,onset,maxOnsetDelay, samplingPeriod):\n",
    "    # eliminate if the onset or peak are too far away from the peak in derivative\n",
    "    toEliminate = []\n",
    "    for i in range(cwtpeaks.size):\n",
    "        if cwtpeaks[i]-onset[i]>maxOnsetDelay/samplingPeriod:#don't know yet how big this hsould be\n",
    "            toEliminate.append(i)\n",
    "        elif peakTime[i]-cwtpeaks[i]>maxPeakDelay/samplingPeriod:\n",
    "            toEliminate.append(i)\n",
    "    return toEliminate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3a8ad3",
   "metadata": {},
   "source": [
    "## CWT approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1feed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,1)\n",
    "for i in pureYvals:\n",
    "    ax.plot(pureYvals[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86b3c89",
   "metadata": {},
   "source": [
    "### compare find_peaks_cwt using different scale vectors\n",
    "\n",
    "Here, I made several side-by-side comparisons of parameter variations"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b5c964b",
   "metadata": {},
   "source": [
    "# compare find_peaks_cwt using different scale vectors\n",
    "scaleParam = {0: 2.**np.arange(0, 1, 1./16.),\n",
    "              1: 2.**np.arange(1, 2, 1./16.),\n",
    "              2: 2.**np.arange(2, 3, 1./16.),\n",
    "              3: 2.**np.arange(3, 4, 1./16.),\n",
    "              4: 2.**np.arange(2, 4, 1./16.),\n",
    "              #5: 2.**np.arange(0, 6, 1./16.)\n",
    "             }\n",
    "f,ax=plt.subplots(5,2)\n",
    "start=0\n",
    "\n",
    "for i,scales in scaleParam.items():\n",
    "    y=pureYvals[2]\n",
    "    detectSignal = signal.medfilt(y,3)\n",
    "    #detectSignal=np.diff(detectSignal)\n",
    "    detectSignal = central_diff_derivative(detectSignal, 0.05, 0.02)\n",
    "    #detectSignal = np.abs(detectSignal)\n",
    "    peaks = signal.find_peaks_cwt(detectSignal, scales, gap_thresh=1, window_size=20, noise_perc=5)\n",
    "    peaks2 = signal.find_peaks_cwt(-detectSignal, scales, gap_thresh=1, window_size=15, noise_perc=5)\n",
    "    peaks = np.concatenate((peaks,peaks2))\n",
    "    #peaks = signal.find_peaks_cwt(y, scales, gap_thresh=3)\n",
    "    #c,fs = pywt.cwt(y, scales, 'mexh', sampling_period=0.02)\n",
    "    c,fs = pywt.cwt(detectSignal, scales, 'mexh', sampling_period=0.02)\n",
    "    ax[i-start,0].plot(y)\n",
    "    ax[i-start,0].plot(detectSignal)\n",
    "    ax[i-start,0].plot(np.mean(c,axis=0))\n",
    "    #if peaks.size==0:\n",
    "    #    continue\n",
    "    ax[i-start,0].plot(peaks, y[peaks], 'rx')\n",
    "    ax[i-start,0].plot(peaks, detectSignal[peaks], 'kx')\n",
    "    #ax[i-start,0].set_title(scales)\n",
    "    im=ax[i-start,1].pcolormesh(c)\n",
    "    im.set_clim([-0.5,0.5])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde61251",
   "metadata": {},
   "source": [
    "### detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed08965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random sample of trials to plot (visually inspect)\n",
    "plotIdx = sorted(np.random.choice(np.arange(anlyz.segments.size), 6, replace=False))\n",
    "plotIdx=np.append(plotIdx, anlyz.segments.size)#append a number that will never be reached later\n",
    "#plotIdx=np.array([0,1,2,3,4,5,6,7,500])\n",
    "plotIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ccad3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "saccDataCWT = {}\n",
    "\n",
    "# scales = 2.**np.arange(1.5, 3., 1./16.)\n",
    "scales = 2.**np.arange(2, 3., 1./16.)\n",
    "diff_half_width = 0.05 #0.05\n",
    "medianFilterWidth = 3\n",
    "cwtpeaks_window_size = 20\n",
    "deviationWindow = 0.16 #0.16\n",
    "deviationLimit = 0.65 # was 0.55 for spont\n",
    "saccAmpThreshold = 20.*np.pi/180\n",
    "minPeakInterval = 0.5\n",
    "cascadeFiltering = True\n",
    "maxPeakDelay = 0.2#s\n",
    "maxOnsetDelay =  0.1#s\n",
    "\n",
    "#f,ax=plt.subplots(5,2)\n",
    "f,ax=plt.subplots(6,1, figsize=(9,16))\n",
    "ax=ax.reshape(-1,1)\n",
    "#start=0\n",
    "pltix=0\n",
    "#for i in range(start,start+5):\n",
    "for i in range(anlyz.segments.size):\n",
    "#for i in [0]:\n",
    "    if not i in pureYvals.keys():    continue\n",
    "    y=pureYvals[i]\n",
    "    detectSignal = signal.medfilt(y,medianFilterWidth)\n",
    "    #detectSignal=np.diff(detectSignal)\n",
    "    detectSignal = central_diff_derivative(detectSignal, diff_half_width, 0.02)\n",
    "    #peaks = signal.find_peaks_cwt(detectSignal, scales, gap_thresh=1, window_size=20, noise_p=5)\n",
    "    peaks = signal.find_peaks_cwt(detectSignal, scales, gap_thresh=1, \n",
    "                                  window_size=cwtpeaks_window_size, noise_perc=5)\n",
    "    peaks2 = signal.find_peaks_cwt(-detectSignal, scales, gap_thresh=1, \n",
    "                                   window_size=cwtpeaks_window_size, noise_perc=5)\n",
    "    peaks = np.concatenate((peaks,peaks2))\n",
    "    peaks.sort() #sorts in-place\n",
    "    peaks = peaks.astype('int')\n",
    "#     print(f'Starting with {peaks.size} peaks')\n",
    "    peakTime = findPeakTimes(np.diff(signal.medfilt(y,medianFilterWidth)), peaks)\n",
    "#     peakTime = findPeakTimes(detectSignal, peaks)+int(diff_half_width/anlyz.kineflyPeriod)\n",
    "#     onsetTime = findOnsetTime(np.diff(signal.medfilt(y,5)), peaks)\n",
    "    onsetTime = findOnsetTime(np.diff(signal.medfilt(y,medianFilterWidth)), peaks)\n",
    "#     onsetTime = findOnsetTime(central_diff_derivative(signal.medfilt(y,medianFilterWidth),0.025,0.02), peaks)\n",
    "#     onsetTime = findOnsetTime(detectSignal, peaks)\n",
    "    saccAmp = getSaccadeAmplitude(y, peakTime, onsetTime, 1/anlyz.kineflyPeriod)\n",
    "\n",
    "    # plot detection before filtering\n",
    "    if i==plotIdx[pltix]:\n",
    "        ax[pltix,0].plot(y)\n",
    "        #ax[i-start,0].plot(detectSignal)\n",
    "        ax[pltix,0].plot(peaks, y[peaks], 'rx')\n",
    "        #ax[i-start,0].plot(peaks, detectSignal[peaks], 'kx')\n",
    "        #im=ax[i-start,1].pcolormesh(c)\n",
    "        #im.set_clim([-0.5,0.5])\n",
    "\n",
    "        ax[pltix,0].plot(peakTime, y[peakTime], 'k^')\n",
    "        ax[pltix,0].plot(onsetTime, y[onsetTime], 'r>')\n",
    "        ax[pltix,0].plot(peakTime, saccAmp, 'ko')\n",
    "\n",
    "    #start filtering stage\n",
    "    keep = np.ones_like(peaks,dtype=bool)\n",
    "    elim = eliminateBySaccAmplitude(saccAmp, saccAmpThreshold)\n",
    "    keep[elim]=False\n",
    "#     print(f'eliminating {len(elim)} elements in this step')\n",
    "    if cascadeFiltering:\n",
    "        peaks = peaks[keep]\n",
    "        peakTime = peakTime[keep]\n",
    "        onsetTime = onsetTime[keep]\n",
    "        saccAmp = saccAmp[keep]\n",
    "        keep = np.ones_like(peaks,dtype=bool)\n",
    "    \n",
    "    #elim = eliminateByPeakProximity(onsetTime, peakTime, samplingRate=anlyz.kineflyPeriod, timeDist=0.2)\n",
    "#     elim = eliminateByPeakProximity2(onsetTime, samplingRate=1/anlyz.kineflyPeriod, timeDist=minPeakInterval)\n",
    "    elim = eliminateByPeakProximity3(onsetTime, saccAmp, samplingRate=1/anlyz.kineflyPeriod,\n",
    "                                     timeDist=minPeakInterval)\n",
    "#     elim = eliminateByPeakProximity4(onsetTime,peakTime, saccAmp, samplingRate=1/anlyz.kineflyPeriod,\n",
    "#                                      timeDist=minPeakInterval)\n",
    "    keep[elim]=False\n",
    "#     print(f'eliminating {len(elim)} elements in this step')\n",
    "    #combine with next step\n",
    "#     if cascadeFiltering:\n",
    "#         peaks = peaks[keep]\n",
    "#         peakTime = peakTime[keep]\n",
    "#         onsetTime = onsetTime[keep]\n",
    "#         saccAmp = saccAmp[keep]\n",
    "#         keep = np.ones_like(peaks,dtype=bool)\n",
    "    #these two steps are similar, therefore combine together\n",
    "    elim = eliminateByDeviation(y, onsetTime, saccAmp, 1/anlyz.kineflyPeriod, \n",
    "                                window=deviationWindow, deviationLimit=deviationLimit)\n",
    "    keep[elim]=False\n",
    "#     print(f'eliminating {len(elim)} elements')\n",
    "    if cascadeFiltering:\n",
    "        peaks = peaks[keep]\n",
    "        peakTime = peakTime[keep]\n",
    "        onsetTime = onsetTime[keep]\n",
    "        saccAmp = saccAmp[keep]\n",
    "        keep = np.ones_like(peaks,dtype=bool)\n",
    "    \n",
    "    \n",
    "    elim = eliminatePutativePersistent(peaks, peakTime,maxPeakDelay, onsetTime,maxOnsetDelay, anlyz.kineflyPeriod)\n",
    "    keep[elim]=False\n",
    "#     print(f'eliminating {len(elim)} elements in this step')\n",
    "    \n",
    "    #theta = extractFlyStatistics(y, 1/anlyz.kineflyPeriod, plot=False)\n",
    "    #print(theta)\n",
    "\n",
    "    if i==plotIdx[pltix]:\n",
    "#         ax[pltix,0].plot(y)\n",
    "#         #ax[i-start,0].plot(detectSignal)\n",
    "#         ax[pltix,0].plot(peaks, y[peaks], 'rx')\n",
    "#         #ax[i-start,0].plot(peaks, detectSignal[peaks], 'kx')\n",
    "#         #im=ax[i-start,1].pcolormesh(c)\n",
    "#         #im.set_clim([-0.5,0.5])\n",
    "\n",
    "#         ax[pltix,0].plot(peakTime, y[peakTime], 'k^')\n",
    "#         ax[pltix,0].plot(onsetTime, y[onsetTime], 'r>')\n",
    "#         ax[pltix,0].plot(peakTime, saccAmp, 'ko')\n",
    "        ax[pltix,0].plot(peaks[keep], y[peaks[keep]], '>',color='b',markerfacecolor=None)\n",
    "        ax[pltix,0].plot(peakTime[keep], y[peakTime[keep]], '^',color='b',markerfacecolor=None)\n",
    "        ax[pltix,0].set_title(plotIdx[pltix])\n",
    "        pltix += 1\n",
    "\n",
    "    peaks = peaks[keep]\n",
    "    peakTime = peakTime[keep]\n",
    "    onsetTime = onsetTime[keep]\n",
    "    saccAmp = saccAmp[keep]\n",
    "    \n",
    "    saccDataCWT[i] = dict([('peakTime',peakTime),('onset',onsetTime), ('saccAmp',saccAmp), ('CWTpeaks',peaks)])\n",
    "detectionParams = {'method':'CWT',\n",
    "                  'scales': scales,\n",
    "                  'diff_half_width': diff_half_width,\n",
    "                  'medianFilterWidth': medianFilterWidth,\n",
    "                  'CWTpeaks_window_size':cwtpeaks_window_size,\n",
    "                   'deviationWindow':deviationWindow,\n",
    "                   'deviationLimit': deviationLimit,\n",
    "                   'saccAmpThreshold': saccAmpThreshold,\n",
    "                   'minPeakInterval':minPeakInterval,\n",
    "                   'maxPeakDelay': maxPeakDelay,\n",
    "                   'maxOnsetDelay': maxOnsetDelay,\n",
    "                   'cascadeFiltering': cascadeFiltering\n",
    "                  }\n",
    "f.suptitle('median filter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09e70d0",
   "metadata": {},
   "source": [
    "### plot"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d178054e",
   "metadata": {},
   "source": [
    "# reading from previously analyzed file\n",
    "anlyz = DummyAnalyzer.retrieveFromNeomatfile(os.path.join(DATAPATH,'spontaneousSaccades','oldData',\n",
    "              'CWTdetection_previous', 'fly1_wtcs_20120217_cell1_oldData_spontSaccades_cwtDetection.neomat'))\n",
    "# anlyz = DummyAnalyzer.retrieveFromNeomatfile(os.path.join(DATAPATH,'spontaneousSaccades',\n",
    "#               'CWTdetection', 'fly3_x21_20200417_cell1_looming_wb_spontSaccades_cwtDetection.neomat'))\n",
    "saccDataCWT = anlyz.saccades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae84c9a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saccDataCWT"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cebc7b64",
   "metadata": {},
   "source": [
    "onset2peak = []\n",
    "for el,v in saccDataCWT.items():\n",
    "    onset2peak.extend(v['peakTime']-v['onset'])\n",
    "px.histogram(onset2peak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3295ce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "allLeft = []\n",
    "allRight = []\n",
    "\n",
    "f,ax=plt.subplots(2,2, figsize=(9,8))\n",
    "# for el in range(11,12):\n",
    "for el in range(len(anlyz)):\n",
    "    #if not el in pureYvals.keys():   continue\n",
    "    if not isinstance(saccDataCWT[el]['peakTime'], (list,np.ndarray)):\n",
    "        saccDataCWT[el] = {k:np.array([v]) for k,v in saccDataCWT[el].items()}\n",
    "    if len(saccDataCWT[el]['peakTime'])==0:  continue\n",
    "    v = saccDataCWT[el]\n",
    "    #actualOnset= (v['onset']*anlyz.meanFramePeriod*anlyz.samplingRate).astype(int)\n",
    "    actualOnset= (v['CWTpeaks']*0.02*anlyz.samplingRate).astype(int)\n",
    "    #pdb.set_trace()\n",
    "    saccAmp = v['saccAmp']\n",
    "    rightSacc = []\n",
    "    leftSacc=[]\n",
    "    #chunk = int(0.5*anlyz.samplingRate)\n",
    "    chunk = int(0.5*anlyz.samplingRate)\n",
    "    preOnset = int(0.25*anlyz.samplingRate)\n",
    "    ephys = anlyz.segments[el].data[0]#ephys data\n",
    "    wba = anlyz.segments[el].data[-1]*180/np.pi\n",
    "    #f,ax=plt.subplots(2,1, figsize=(10,4))\n",
    "    #ax[0].plot(wba)\n",
    "    #ax[1].plot(ephys)\n",
    "    #for o,a in zip(actualOnset, saccAmp):\n",
    "    for j in range(saccAmp.size):\n",
    "        o=actualOnset[j]\n",
    "        a=saccAmp[j]\n",
    "        p= (v['peakTime']*0.02*anlyz.samplingRate).astype(int)[j]\n",
    "        #ax[0].plot(o,wba[o], 'r>')\n",
    "        #ax[0].plot(p, wba[p],'k^' )\n",
    "        #xval = np.arange(o-preOnset, o+chunk)/anlyz.samplingRate\n",
    "        if a<0:\n",
    "            leftSacc.append(ephys[o-preOnset:o+chunk])\n",
    "            #leftSacc.append(ephys[o-preOnset:p+chunk])\n",
    "            d = wba[o-preOnset:o+chunk]\n",
    "            xval = np.arange(-preOnset, d.size-preOnset)/anlyz.samplingRate\n",
    "            ax[0,1].plot(xval, d-np.mean(d[:int(0.25*anlyz.samplingRate)]))#have to adjust this for PWC-fitted data\n",
    "            d = ephys[o-preOnset:o+chunk]\n",
    "            ax[1,1].plot(xval, d-np.mean(d[:int(0.25*anlyz.samplingRate)]))\n",
    "        else:\n",
    "            rightSacc.append(ephys[o-preOnset:o+chunk])\n",
    "            #rightSacc.append(ephys[o-preOnset:p+chunk])\n",
    "            d = wba[o-preOnset:o+chunk]\n",
    "            xval = np.arange(-preOnset,d.size-preOnset)/anlyz.samplingRate\n",
    "            ax[0,0].plot(xval, d-np.mean(d[:int(0.25*anlyz.samplingRate)]))#have to adjust this for PWC-fitted data\n",
    "            d = ephys[o-preOnset:o+chunk]\n",
    "            ax[1,0].plot(xval, d-np.mean(d[:int(0.25*anlyz.samplingRate)]))\n",
    "    allLeft.extend(leftSacc)\n",
    "    allRight.extend(rightSacc)\n",
    "ax[0,0].set_title('saccades to right')\n",
    "ax[0,1].set_title('saccades to left')\n",
    "ax[0,0].set_ylabel('L-R WBA')\n",
    "ax[1,0].set_ylabel('U [mV]')\n",
    "ax[1,0].set_xlabel('time [s]')\n",
    "ax[1,1].set_xlabel('time [s]')\n",
    "#ax[0,0].set_ylim([-0.15, 0.51])\n",
    "#ax[0,1].set_ylim([-0.51, 0.15])\n",
    "#ax[1,0].set_ylim([-4, 4])\n",
    "#ax[1,1].set_ylim([-4, 4])\n",
    "#ax[0,0].set_ylim([-0.21, 0.45])\n",
    "[a.set_xlim([-0.25,0.5]) for a in ax.flatten()]\n",
    "\n",
    "[a.axvline(0., color='k', linestyle=':') for a in ax.flatten()]    \n",
    "\n",
    "# post-hoc adding a mean\n",
    "i=0\n",
    "for a in ax.flatten():\n",
    "    ally=[c.get_data()[1] for c in a.get_children() if isinstance(c, plt.Line2D)]\n",
    "    ally=[y for y in ally if len(y)>100]\n",
    "    #print([len(y) for y in ally])\n",
    "    try:\n",
    "        minLen = min([len(y) for y in ally])\n",
    "    #     padLen = int(np.median([len(y) for y in ally])) # !why median!?\n",
    "        padLen = int(np.max([len(y) for y in ally]))\n",
    "        #print(minLen)\n",
    "        ally = [np.pad(y,(0,padLen-len(y)), mode='constant', constant_values=np.nan) for y in ally]\n",
    "        #ally = np.array([y[:minLen] for y in ally])\n",
    "        ally = np.array(ally)\n",
    "        a.plot(np.arange(-preOnset,ally.shape[1]-preOnset)/anlyz.samplingRate, np.nanmean(ally,axis=0), 'k-')\n",
    "\n",
    "    #     mu = np.nanmean(ally,axis=0)\n",
    "    #     a.plot(np.arange(-preOnset,ally.shape[1]-preOnset)/anlyz.samplingRate, (mu-np.nanmean(mu))/np.nanstd(mu), 'r-')\n",
    "\n",
    "        if i==0:\n",
    "            anlyz.saccRightMeanWBA = np.nanmean(ally,axis=0)\n",
    "        elif i==1:\n",
    "            anlyz.saccLeftMeanWBA = np.nanmean(ally,axis=0)\n",
    "        elif i==2:\n",
    "            anlyz.saccRightMeanEphys = np.nanmean(ally,axis=0)\n",
    "        elif i==3:\n",
    "            anlyz.saccLeftMeanEphys = np.nanmean(ally,axis=0)\n",
    "        i+=1\n",
    "    except ValueError: #if min() arg is an empty sequence\n",
    "        ally = np.zeros(0)\n",
    "        \n",
    "    #a.set_xlim([-preOnset,padLen/anlyz.samplingRate])\n",
    "    a.text(0.05, 0.95, f'n={ally.shape[0]}', transform=a.transAxes, fontsize=14,\n",
    "    verticalalignment='top')\n",
    "\n",
    "f.suptitle(anlyz.flyID[0]+'_'+anlyz.metadata[0]['HS_type'])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35f54f29",
   "metadata": {},
   "source": [
    "mu = np.nanmean(ally,axis=0)\n",
    "a.plot(np.arange(-preOnset,ally.shape[1]-preOnset)/anlyz.samplingRate, (mu-np.nanmean(mu))/np.nanstd(mu), 'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc32575",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(detectionParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9eaf13",
   "metadata": {},
   "source": [
    "### save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7e9718",
   "metadata": {},
   "outputs": [],
   "source": [
    "anlyz.flyID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2163214",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.savefig(os.path.join(DATAPATH,'spontaneousSaccades','oldData',\n",
    "                       'CWTdetection', anlyz.flyID[0]+'_oldData_spontSaccades.png' ))\n",
    "f.savefig(os.path.join(DATAPATH,'spontaneousSaccades','oldData',\n",
    "                       'CWTdetection', anlyz.flyID[0]+'_oldData_spontSaccades.svg' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b62aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into saccades left and right\n",
    "# make use of the mechanism for splitting flying or not\n",
    "anlyz.saccades = saccDataCWT\n",
    "anlyz.write2neomatfile(os.path.join(DATAPATH,'spontaneousSaccades','oldData',\n",
    "                            'CWTdetection', anlyz.flyID[0]+'_oldData_spontSaccades_cwtDetection.neomat'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb12b395",
   "metadata": {},
   "source": [
    "### quality control: plot saccades over time"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3874938",
   "metadata": {},
   "source": [
    "f,ax=plt.subplots(1,1)\n",
    "for el in range(len(anlyz)):\n",
    "    for sacc in range(len(saccDataCWT[el]['peakTime'])):\n",
    "#         if anlyz.segments[el].boundaries[0]>100e4: continue\n",
    "        ax.plot(anlyz.segments[el].data[-1])\n",
    "        print(anlyz.segments[el].boundaries, saccDataCWT[el]['onset'][sacc], saccDataCWT[el]['saccAmp'][sacc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d6ff3a",
   "metadata": {},
   "source": [
    "## plot saccades in whole record context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07db2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original file if necessary\n",
    "try:\n",
    "    loader\n",
    "    assert(loader.file==anlyz.fileOrigin[0])\n",
    "except (NameError, AssertionError): #if loader doesn't exist\n",
    "    loader = bc.DataLoader(anlyz.fileOrigin[0])\n",
    "print(loader.file)\n",
    "print(anlyz.flyID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8faef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "colorSaccLeft = 'g'\n",
    "colorSaccRight = 'y'\n",
    "\n",
    "# saccDataCWT = anlyz.saccades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d92b43",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allWBA = loader.data[1]-loader.data[2]\n",
    "allEphys = loader.data[0]\n",
    "reductionFactor = 10\n",
    "\n",
    "f,ax=plt.subplots(2,1, figsize=(9,6))\n",
    "timeax = np.arange(0,loader.data.shape[1],reductionFactor)/anlyz.samplingRate\n",
    "ax[0].plot(timeax, allWBA[::reductionFactor])\n",
    "ax[1].plot(timeax, allEphys[::reductionFactor])\n",
    "ax[0].plot(timeax, loader.data[3,::reductionFactor], color=[.6,.6,.6], alpha=.6)\n",
    "ax[1].plot(timeax, loader.data[3,::reductionFactor]*8-45, color=[.6,.6,.6], alpha=.6)\n",
    "\n",
    "\n",
    "for el in range(len(anlyz)):\n",
    "    #if not el in pureYvals.keys():   continue\n",
    "    if not isinstance(saccDataCWT[el]['peakTime'], (list,np.ndarray)):\n",
    "        saccDataCWT[el] = {k:np.array([v]) for k,v in saccDataCWT[el].items()}\n",
    "    if len(saccDataCWT[el]['peakTime'])==0:  continue\n",
    "    v = saccDataCWT[el]\n",
    "    #actualOnset= (v['onset']*anlyz.meanFramePeriod*anlyz.samplingRate).astype(int)\n",
    "    actualOnset= (v['CWTpeaks']*0.02*anlyz.samplingRate).astype(int)\n",
    "    #pdb.set_trace()\n",
    "    saccAmp = v['saccAmp']\n",
    "    chunk = int(0.5*anlyz.samplingRate)\n",
    "    preOnset = int(0.15*anlyz.samplingRate)\n",
    "    for j in range(saccAmp.size):\n",
    "        p= (v['peakTime']*0.02*anlyz.samplingRate).astype(int)[j]\n",
    "        if saccAmp[j]<0:\n",
    "            plotColor=colorSaccLeft\n",
    "        else:\n",
    "            plotColor=colorSaccRight\n",
    "        xval = np.arange(-preOnset, chunk)#/anlyz.samplingRate\n",
    "        xval += actualOnset[j]\n",
    "        xval += anlyz.segments[el].boundaries[0]\n",
    "        xval = xval.astype(int)\n",
    "        ax[0].plot(xval/anlyz.samplingRate, allWBA[xval], color=plotColor)\n",
    "        ax[1].plot(xval/anlyz.samplingRate, allEphys[xval], color=plotColor)\n",
    "        ax[0].axvline((actualOnset[j]+anlyz.segments[el].boundaries[0])/anlyz.samplingRate, linestyle=':',color='k')\n",
    "        ax[1].axvline((actualOnset[j]+anlyz.segments[el].boundaries[0])/anlyz.samplingRate, linestyle=':',color='k')\n",
    "f.suptitle(anlyz.flyID[0])\n",
    "ax[0].set_ylabel('L-R WBA')\n",
    "ax[1].set_ylabel('Vm')\n",
    "# ax[0].set_xlim([0, loader.data.shape[1]])\n",
    "# ax[1].set_xlim([0, loader.data.shape[1]])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4bb0f64f",
   "metadata": {},
   "source": [
    "f.savefig(os.path.join(DATAPATH,'spontaneousSaccades','oldData',\n",
    "                       'CWTdetection', anlyz.flyID[0]+'_oldData_spontSaccades_contextView.svg' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e528e0",
   "metadata": {},
   "source": [
    "# variability of response of single HS cell\n",
    "i.e. if an HS cell can be counted as hyperpolarizing, does it always hyperpolarize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c92ebda",
   "metadata": {},
   "source": [
    "## load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a065e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5466c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = glob.glob(os.path.join(DATAPATH,'spontaneousSaccades','oldData','CWTdetection','*.neomat'))\n",
    "# flist.extend([os.path.join(DATAPATH,'spontaneousSaccades','CWTdetection',\n",
    "#       f'{fn}_looming_wb_spontSaccades_cwtDetection.neomat') \n",
    "#      for fn in ('fly4_x21_20200420_cell1', \n",
    "#                 'fly3_x21_20200415_cell1', \n",
    "#                 'fly3_x21_20200417_cell1')]) # missing 2020_09_30\n",
    "flist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec471910",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn=flist[0]\n",
    "da = DummyAnalyzer.retrieveFromNeomatfile(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10195c92",
   "metadata": {},
   "source": [
    "## plot all HS cells' distribution of spontaneous saccades' Vm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc88aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "i,segment=list(da.saccades.items())[-1]\n",
    "i,segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9a6e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# going through a single file and segment to test the procedure\n",
    "\n",
    "wba=da.segments[i].data[-1]\n",
    "# wba=ndimage.gaussian_filter1d(wba,60)\n",
    "eph=da.segments[i].data[0]\n",
    "eph=ndimage.gaussian_filter1d(eph,60)\n",
    "\n",
    "plotrange = np.arange(int(-0.15*10000),int(0.5*10000))\n",
    "f,ax=plt.subplots(2,2)\n",
    "for ix in range(len((da.saccades[i]['peakTime']))):\n",
    "    on = da.saccades[i]['onset'][ix]\n",
    "    pt = da.saccades[i]['peakTime'][ix]+1\n",
    "    offWba =np.mean(wba[int(on*da.kineflyPeriod*da.samplingRate)+np.arange(-0.15*da.samplingRate,0, dtype=int)])\n",
    "    sa = wba[int(pt*da.kineflyPeriod*da.samplingRate)] - offWba\n",
    "    print(sa)\n",
    "    idx = int(np.argmax(signal.medfilt(np.abs(eph[int(on*da.kineflyPeriod*da.samplingRate):int(pt*da.kineflyPeriod*da.samplingRate)]), 101))\n",
    "              +on*da.kineflyPeriod*da.samplingRate)\n",
    "    offEph = np.mean(eph[int(on*da.kineflyPeriod*da.samplingRate)+np.arange(-0.15*da.samplingRate,0, dtype=int)])\n",
    "#     pa = eph[idx] - offEph\n",
    "    pa = np.mean(eph[idx-100:idx+400]) - offEph\n",
    "    \n",
    "    ax[0,0].plot(plotrange, wba[(on*da.kineflyPeriod*da.samplingRate+plotrange).astype(int)] )\n",
    "    ax[0,0].plot((pt-on)*da.kineflyPeriod*da.samplingRate, wba[int(pt*da.kineflyPeriod*da.samplingRate)], 'o')\n",
    "    \n",
    "    ax[0,1].plot(plotrange, eph[(on*da.kineflyPeriod*da.samplingRate+plotrange).astype(int)])\n",
    "    ax[0,1].plot(idx-on*da.kineflyPeriod*da.samplingRate, eph[idx], 'o')\n",
    "#     ax[0,1].plot((pt-on)*da.kineflyPeriod*da.samplingRate, eph[int(pt*da.kineflyPeriod*da.samplingRate)], 'o')\n",
    "    ax[1,0].plot(plotrange, wba[(on*da.kineflyPeriod*da.samplingRate+plotrange).astype(int)] - offWba )\n",
    "    ax[1,0].plot((pt-on)*da.kineflyPeriod*da.samplingRate, sa, 'o')\n",
    "    \n",
    "    ax[1,1].plot(plotrange, eph[(on*da.kineflyPeriod*da.samplingRate+plotrange).astype(int)]-offEph)\n",
    "    ax[1,1].plot(idx-on*da.kineflyPeriod*da.samplingRate, pa, 'o')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d674e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "allPolAmp = {}\n",
    "allSaccAmp = {}\n",
    "flyidlist=[]\n",
    "\n",
    "for fn in flist[:]:\n",
    "    da = DummyAnalyzer.retrieveFromNeomatfile(fn)\n",
    "    if 'oldData' not in fn:\n",
    "        print('multiplying WBA at',fn)\n",
    "        da.saccLeftMeanWBA *= (180/np.pi)\n",
    "        da.saccRightMeanWBA *= (180/np.pi)\n",
    "    else:\n",
    "        da.saccLeftMeanWBA /= 1.7\n",
    "        da.saccRightMeanWBA /= 1.7\n",
    "    if max(abs(da.saccLeftMeanEphys)) <0.1:\n",
    "        print('multiplying Vm at',fn)\n",
    "        da.saccLeftMeanEphys *= 100.\n",
    "        da.saccRightMeanEphys *= 100.\n",
    "    if abs(da.samplingRate -10000) >10:\n",
    "        raise Exception('unexpected sampling rate')\n",
    "    # go through all sacccades, extract amplitudes\n",
    "    fid = da.flyID[0]\n",
    "    flyidlist.append(fid)\n",
    "    saccAmp = []\n",
    "    polAmp=[]\n",
    "    for i,segment in da.saccades.items():\n",
    "        wba=da.segments[i].data[da.channelmap['wingDiff']]\n",
    "        eph = ndimage.gaussian_filter1d(da.segments[i].data[da.channelmap['ephys']], 150)\n",
    "        if isinstance(da.saccades[i]['peakTime'], (int,float)):\n",
    "            da.saccades[i]['peakTime'] = [ da.saccades[i]['peakTime'] ]\n",
    "            da.saccades[i]['onset'] = [ da.saccades[i]['onset'] ]\n",
    "        for j in range(len((da.saccades[i]['peakTime']))):\n",
    "            on = da.saccades[i]['onset'][j]\n",
    "            pt = da.saccades[i]['peakTime'][j]+1\n",
    "            sa = wba[int(pt*da.kineflyPeriod*da.samplingRate)] - np.mean(wba[int(on*da.kineflyPeriod*da.samplingRate)+np.arange(-0.15*da.samplingRate,0, dtype=int)])\n",
    "#             idx = int(np.argmax(signal.medfilt(np.abs(eph[int(on*da.kineflyPeriod*da.samplingRate):int(pt*da.kineflyPeriod*da.samplingRate)]), 101))\n",
    "            idx = int(np.argmax(signal.medfilt(np.abs(eph[int(on*da.kineflyPeriod*da.samplingRate):int((on+6)*da.kineflyPeriod*da.samplingRate)]), 101))\n",
    "                      +on*da.kineflyPeriod*da.samplingRate)\n",
    "            pa = eph[idx] - np.mean(eph[int(on*da.kineflyPeriod*da.samplingRate)+np.arange(-0.15*da.samplingRate,0, dtype=int)])\n",
    "            #pa = np.mean(eph[idx-200:idx+200]) - np.mean(eph[int(on*da.kineflyPeriod*da.samplingRate)+np.arange(-0.15*da.samplingRate,0, dtype=int)])\n",
    "            saccAmp.append(sa)\n",
    "            polAmp.append(pa)\n",
    "    allPolAmp[fid] = polAmp\n",
    "    allSaccAmp[fid] = saccAmp\n",
    "#     allSaccLeftMeanEphys[da.flyID[0]] = da.saccLeftMeanEphys[-int(0.65*da.samplingRate):]\n",
    "#     allSaccRightMeanEphys[da.flyID[0]] = da.saccRightMeanEphys[-int(0.65*da.samplingRate):]\n",
    "#     allSaccLeftMeanWBA[da.flyID[0]] = da.saccLeftMeanWBA[-int(0.65*da.samplingRate):]\n",
    "#     allSaccRightMeanWBA[da.flyID[0]] = da.saccRightMeanWBA[-int(0.65*da.samplingRate):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8862adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into saccades to the left or right\n",
    "saccL_pol = {}\n",
    "saccL_beh = {}\n",
    "saccR_pol = {}\n",
    "saccR_beh = {}\n",
    "for fid in allPolAmp:\n",
    "    polamp = np.array(allPolAmp[fid])\n",
    "    saccamp = allSaccAmp[fid]\n",
    "    saccamp = np.array(saccamp)\n",
    "    saccL_pol[fid] = polamp[saccamp<0]\n",
    "    saccL_beh[fid] = saccamp[saccamp<0]\n",
    "    saccR_pol[fid] = polamp[saccamp>0]\n",
    "    saccR_beh[fid] = saccamp[saccamp>0]\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "64bc99f9",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# now plot results in some way\n",
    "f,ax=plt.subplots()\n",
    "plt.boxplot(list(saccL_pol.values()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(4,5)\n",
    "ax=ax.flatten()\n",
    "for i, fid in enumerate(allPolAmp.keys()):\n",
    "    pa=allPolAmp[fid]\n",
    "    sa=allSaccAmp[fid]\n",
    "    if fid in subtype_dict['HSN']:\n",
    "        c=plt.cm.Purples(0.9)\n",
    "    elif fid in subtype_dict['HSE']:\n",
    "        c=plt.cm.YlGn(0.8)\n",
    "    elif fid in subtype_dict['HSS']:\n",
    "        c=plt.cm.pink(0.25)\n",
    "    else:\n",
    "        c='blue'\n",
    "    ax[i].scatter(sa,pa, color=c)\n",
    "    ax[i].axvline(0)\n",
    "    ax[i].axhline(0)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b744d3",
   "metadata": {},
   "source": [
    "# gather results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9176c6",
   "metadata": {},
   "source": [
    "## load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f877ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6da9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = glob.glob(os.path.join(DATAPATH,'spontaneousSaccades','oldData','CWTdetection','*.neomat'))\n",
    "flist.extend([os.path.join(DATAPATH,'spontaneousSaccades','CWTdetection',\n",
    "      f'{fn}_looming_wb_spontSaccades_cwtDetection.neomat') \n",
    "     for fn in ('fly4_x21_20200420_cell1', \n",
    "                'fly3_x21_20200415_cell1', \n",
    "                'fly3_x21_20200417_cell1')]) # missing 2020_09_30\n",
    "flist"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b78d092",
   "metadata": {},
   "source": [
    "ownData = [os.path.join(DATAPATH,'spontaneousSaccades','CWTdetection',name+'.neomat') \n",
    "           for name in ('fly3_x21_20200417_cell1_looming_wb_spontSaccades_cwtDetection',\n",
    "                        'fly4_x21_20200420_cell1_looming_wb_spontSaccades_cwtDetection',\n",
    "                       ) ]\n",
    "flist.extend(ownData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7282d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn=flist[0]\n",
    "da = DummyAnalyzer.retrieveFromNeomatfile(fn)\n",
    "f,ax=plt.subplots(1,1)\n",
    "ax.plot(da.saccLeftMeanWBA)\n",
    "ax.plot(da.saccRightMeanWBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e3c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "allSaccLeftMeanEphys = {}\n",
    "allSaccLeftMeanWBA = {}\n",
    "allSaccRightMeanEphys = {}\n",
    "allSaccRightMeanWBA = {}\n",
    "\n",
    "for fn in flist[:]:\n",
    "    da = DummyAnalyzer.retrieveFromNeomatfile(fn)\n",
    "    if 'oldData' not in fn:\n",
    "        print('multiplying WBA at',fn)\n",
    "        da.saccLeftMeanWBA *= (180/np.pi)\n",
    "        da.saccRightMeanWBA *= (180/np.pi)\n",
    "    else:\n",
    "        da.saccLeftMeanWBA /= 1.7\n",
    "        da.saccRightMeanWBA /= 1.7\n",
    "    if max(abs(da.saccLeftMeanEphys)) <0.1:\n",
    "        print('multiplying Vm at',fn)\n",
    "        da.saccLeftMeanEphys *= 100.\n",
    "        da.saccRightMeanEphys *= 100.\n",
    "    if abs(da.samplingRate -10000) >10:\n",
    "        raise Exception('unexpected sampling rate')\n",
    "#         da.saccLeftMeanEphys = signal.resample(da.saccLeftMeanEphys, int(da.saccLeftMeanEphys.size/da.samplingRate*10000.))\n",
    "#         da.saccRightMeanEphys = signal.resample(da.saccRightMeanEphys, int(da.saccRightMeanEphys.size/da.samplingRate*10000.))\n",
    "#         da.saccLeftMeanWBA = signal.resample(da.saccLeftMeanWBA, int(da.saccLeftMeanWBA.size/da.samplingRate*10000.))\n",
    "#         da.saccRightMeanWBA = signal.resample(da.saccRightMeanWBA, int(da.saccRightMeanWBA.size/da.samplingRate*10000.))\n",
    "    allSaccLeftMeanEphys[da.flyID[0]] = da.saccLeftMeanEphys[-int(0.65*da.samplingRate):]\n",
    "    allSaccRightMeanEphys[da.flyID[0]] = da.saccRightMeanEphys[-int(0.65*da.samplingRate):]\n",
    "    allSaccLeftMeanWBA[da.flyID[0]] = da.saccLeftMeanWBA[-int(0.65*da.samplingRate):]\n",
    "    allSaccRightMeanWBA[da.flyID[0]] = da.saccRightMeanWBA[-int(0.65*da.samplingRate):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabfdb79",
   "metadata": {},
   "source": [
    "## allocate colors and plot all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddeadc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "polAmp=[]\n",
    "saccAmp=[]\n",
    "\n",
    "plotlist = list(allSaccRightMeanEphys.keys())[:]\n",
    "\n",
    "for flyid in plotlist[:]:\n",
    "    #f,ax = plt.subplots(1,2)\n",
    "    \n",
    "    d=allSaccLeftMeanEphys[flyid]\n",
    "    #ax[0].plot(d)\n",
    "    d = ndimage.gaussian_filter1d(d, 60)\n",
    "    #ax[0].plot(d)\n",
    "    ix = int(np.argmax(signal.medfilt(np.abs(d[int(0.15*10000):int(0.35*10000)]), 101))+0.15*10000)\n",
    "    #ax[0].plot(ix, d[ix], 'ro')\n",
    "    polAmp.append(d[ix])\n",
    "\n",
    "    d=allSaccLeftMeanWBA[flyid]\n",
    "    #ax[1].plot(d)\n",
    "    #d = signal.medfilt(d, 201)\n",
    "    d = ndimage.gaussian_filter1d(d, 60)\n",
    "    #ax[1].plot(d)\n",
    "    ix = int(np.argmax(signal.medfilt(np.abs(d[int(0.15*10000):int(0.35*10000)]), 101))+0.15*10000)\n",
    "    #ax[1].plot(ix, d[ix], 'ro')\n",
    "    saccAmp.append(d[ix])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "079cca97",
   "metadata": {},
   "source": [
    "anlyz.saccLeftMeanWBA *= (180/np.pi)\n",
    "anlyz.saccRightMeanWBA *= (180/np.pi)\n",
    "anlyz.write2neomatfile(os.path.join(DATAPATH,'spontaneousSaccades','oldData',\n",
    "                    'CWTdetection',f'{anlyz.flyID[0]}_oldData_spontSaccades_cwtDetection.neomat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4947f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(2,2, figsize=(9,6))\n",
    "plotlist = list(allSaccRightMeanEphys.keys())[:]\n",
    "#plotcolors = plt.cm.BrBG(np.linspace(0,1,len(plotlist)))\n",
    "# plotcolors = plt.cm.viridis((polAmp-min(polAmp))/(max(polAmp)-min(polAmp)))\n",
    "plotcolors = plt.cm.viridis(np.linspace(0,1,len(plotlist)))[np.searchsorted(np.sort(polAmp), polAmp)]\n",
    "i=-1\n",
    "for k in plotlist:\n",
    "    print(k)\n",
    "    i+=1\n",
    "#     if max(abs(allSaccLeftMeanWBA[k])) < 8:\n",
    "#         print('left out', k)\n",
    "#         continue\n",
    "    ax[0,0].plot(np.arange(allSaccLeftMeanWBA[k].size)/10000.-0.15, allSaccLeftMeanWBA[k], color=plotcolors[i], linewidth=0.8)\n",
    "    ax[0,1].plot(np.arange(allSaccRightMeanWBA[k].size)/10000.-0.15, allSaccRightMeanWBA[k], color=plotcolors[i], linewidth=0.8)\n",
    "    ax[1,0].plot(np.arange(allSaccLeftMeanEphys[k].size)/10000.-0.15, allSaccLeftMeanEphys[k], color=plotcolors[i])\n",
    "    ax[1,1].plot(np.arange(allSaccRightMeanEphys[k].size)/10000.-0.15, allSaccRightMeanEphys[k], color=plotcolors[i])\n",
    "ax[0,0].set_title('sacc left WBA')\n",
    "ax[0,1].set_title('sacc right WBA')\n",
    "ax[1,0].set_title('sacc left ephys')\n",
    "ax[1,1].set_title('sacc right ephys')\n",
    "ax[0,0].set_ylabel('$\\Delta$ WBA [°]')\n",
    "ax[1,0].set_ylabel('membrane potential [mV]')\n",
    "[a.axvline(0, linestyle=':', color='k') for a in ax.flatten()]\n",
    "[a.set_xlim([-0.15,0.51]) for a in ax.flatten()]\n",
    "ax[1,0].set_xlabel('time from onset [s]')\n",
    "ax[1,1].set_xlabel('time from onset [s]')\n",
    "# ax[0,0].legend(plotlist, framealpha=0.4)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd56162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a line for average\n",
    "\n",
    "lw=3\n",
    "padLen = max([v.size  for k,v in allSaccLeftMeanWBA.items()])\n",
    "allTraces = np.array([np.pad(v,(0,padLen-v.size), 'constant', constant_values=np.nan) for _,v in allSaccLeftMeanWBA.items()])\n",
    "ax[0,0].plot(np.arange(padLen)/10000.-0.15, np.nanmean(allTraces,axis=0), 'k-', linewidth=lw)\n",
    "\n",
    "padLen = max([v.size  for k,v in allSaccRightMeanWBA.items()])\n",
    "allTraces = np.array([np.pad(v,(0,padLen-v.size), 'constant', constant_values=np.nan) for _,v in allSaccRightMeanWBA.items()])\n",
    "ax[0,1].plot(np.arange(padLen)/10000.-0.15, np.nanmean(allTraces,axis=0), 'k-', linewidth=lw)\n",
    "\n",
    "padLen = max([v.size  for k,v in allSaccLeftMeanEphys.items()])\n",
    "allTraces = np.array([np.pad(v,(0,padLen-v.size), 'constant', constant_values=np.nan) for _,v in allSaccLeftMeanEphys.items()])\n",
    "ax[1,0].plot(np.arange(padLen)/10000.-0.15, np.nanmean(allTraces,axis=0), 'k-', linewidth=lw)\n",
    "\n",
    "padLen = max([v.size  for k,v in allSaccRightMeanEphys.items()])\n",
    "allTraces = np.array([np.pad(v,(0,padLen-v.size), 'constant', constant_values=np.nan) for _,v in allSaccRightMeanEphys.items()])\n",
    "ax[1,1].plot(np.arange(padLen)/10000.-0.15, np.nanmean(allTraces,axis=0), 'k-', linewidth=lw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a7f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "f.savefig(os.path.join(DATAPATH, 'spontaneousSaccades','oldData', \n",
    "                'cwtSorting_oldData_spontanSacc_perFly_withAvg.png'))\n",
    "f.savefig(os.path.join(DATAPATH, 'spontaneousSaccades','oldData', \n",
    "                'cwtSorting_oldData_spontanSacc_perFly_withAvg.svg'))\n",
    "# f.savefig(os.path.join(DATAPATH, 'spontaneousSaccades','oldData', \n",
    "#                 'cwtSorting_oldData_spontanSacc_perFly_withLegend_withAvg.png'))\n",
    "# f.savefig(os.path.join(DATAPATH, 'spontaneousSaccades','oldData', \n",
    "#                 'cwtSorting_oldData_spontanSacc_perFly_withLegend_withAvg.svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6fca34",
   "metadata": {},
   "source": [
    "## plot subsets in different color families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4c7f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtype_dict = {'HSN': ('fly1_wtcs_20111222_cell1',\n",
    "                        #'fly1_wtcs_20120203_cell1',\n",
    "                        'fly1_wtcs_20120207_cell1',\n",
    "                        #'fly1_wtcs_20120208_cell1',\n",
    "                        'fly1_wtcs_20120504_cell1',\n",
    "                        'fly1_wtcs_20120508_cell1',\n",
    "                        'fly1_wtcs_20120418_cell2',\n",
    "                        'fly1_wtcs_20130506_cell1',\n",
    "                        #'fly4_looming_20200420_cell1'\n",
    "                       ),\n",
    "               'HSE': ('fly1_wtcs_20120104_cell1',\n",
    "                       'fly1_wtcs_20120105_cell1',\n",
    "                       'fly1_wtcs_20120501_cell1',\n",
    "                       #'fly1_wtcs_20120103_cell2',\n",
    "                       'fly1_wtcs_20120105_cell2',\n",
    "                       #'fly1_wtcs_20120501_cell2',\n",
    "                       'fly1_wtcs_20120524_cell2',\n",
    "                       'fly1_wtcs_20130502_cell2',\n",
    "                       'fly1_wtcs_20130513_cell1'),\n",
    "               'HSS': (#'fly1_wtcs_20120109_cell1',\n",
    "                       'fly1_wtcs_20120605_cell1',\n",
    "                       'fly1_wtcs_20120104_cell2',\n",
    "                       'fly1_wtcs_20130211_cell1',\n",
    "                       'fly1_wtcs_20130424_cell1',\n",
    "                       #'fly3_looming_20200415_cell1',\n",
    "                       #'fly3_looming_20200417_cell1'\n",
    "                       )\n",
    "               }\n",
    "# define several outliers that are eitehr extremely low or high saccade amplitude, identified visually\n",
    "behavioral_outliers = ('fly1_wtcs_20111222_cell1',#HSN\n",
    "                       'fly3_looming_20200417_cell1',#HSS\n",
    "                       'fly1_wtcs_20120103_cell2')#HSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8366234",
   "metadata": {},
   "source": [
    "### ephys"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c46d86c",
   "metadata": {},
   "source": [
    "# simpler version or first look\n",
    "import plotly.express as px\n",
    "px.line(allSaccLeftMeanEphys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff0672",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,1, figsize=(7,5))\n",
    "\n",
    "plotlist = subtype_dict['HSE']\n",
    "\n",
    "plotcolors = plt.cm.YlGn(np.linspace(0.2,0.7,len(plotlist)))\n",
    "i=-1\n",
    "for k in plotlist:\n",
    "#     if k in behavioral_outliers:    continue\n",
    "    print(k)\n",
    "    data = allSaccLeftMeanEphys[k]\n",
    "    i+=1\n",
    "    ax.plot(np.arange(data.size)/10000.-0.15, data, color=plotcolors[i], linewidth=1., label=k)\n",
    "# ax.legend()\n",
    "# add average line\n",
    "minLen = min([allSaccLeftMeanEphys[fid].size for fid in plotlist])\n",
    "alldata = np.array([allSaccLeftMeanEphys[fid][:minLen] for fid in plotlist])\n",
    "avg_HSE = np.mean(alldata, axis=0)\n",
    "std_HSE = np.std(alldata, axis=0)/len(plotlist)\n",
    "# ax.plot(np.arange(avg_HSE.size)/10000.-0.15,avg_HSE, color=plt.cm.YlGn(0.85), linewidth=2)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6770a283",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotlist = subtype_dict['HSS']\n",
    "\n",
    "plotcolors = plt.cm.pink(np.linspace(0.3,0.4,len(plotlist)))\n",
    "i=-1\n",
    "for k in plotlist:\n",
    "#     if k in behavioral_outliers:    continue\n",
    "    print(k)\n",
    "    data = allSaccLeftMeanEphys[k]\n",
    "    i+=1\n",
    "    ax.plot(np.arange(data.size)/10000.-0.15, data, color=plotcolors[i], linewidth=1., label=k)\n",
    "# ax.legend()\n",
    "# add average line\n",
    "minLen = min([allSaccLeftMeanEphys[fid].size for fid in plotlist])\n",
    "alldata = np.array([allSaccLeftMeanEphys[fid][:minLen] for fid in plotlist])\n",
    "avg_HSS = np.mean(alldata, axis=0)\n",
    "std_HSS = np.std(alldata, axis=0)/len(plotlist)\n",
    "# ax.plot(np.arange(avg_HSS.size)/10000.-0.15,avg_HSS, color=plt.cm.Purples(0.98), linewidth=2)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7275d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotlist = subtype_dict['HSN']\n",
    "\n",
    "plotcolors = plt.cm.Purples(np.linspace(0.2,0.7,len(plotlist)))\n",
    "i=-1\n",
    "for k in plotlist:\n",
    "#     if k in behavioral_outliers:    continue\n",
    "    print(k)\n",
    "    data = allSaccLeftMeanEphys[k]\n",
    "    i+=1\n",
    "    ax.plot(np.arange(data.size)/10000.-0.15, data, color=plotcolors[i], linewidth=1., label=k)\n",
    "# ax.legend()\n",
    "# add average line\n",
    "minLen = min([allSaccLeftMeanEphys[fid].size for fid in plotlist])\n",
    "alldata = np.array([allSaccLeftMeanEphys[fid][:minLen] for fid in plotlist])\n",
    "avg_HSN = np.mean(alldata, axis=0)\n",
    "std_HSN = np.std(alldata, axis=0)/len(plotlist)\n",
    "# ax.plot(np.arange(avg_HSN.size)/10000.-0.15,avg_HSN, color=plt.cm.Purples(0.98), linewidth=2)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0480a738",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.plot(np.arange(avg_HSS.size)/10000.-0.15,avg_HSS, color=plt.cm.pink(0.2), linewidth=2, label='mean HSS')\n",
    "ax.plot(np.arange(avg_HSE.size)/10000.-0.15,avg_HSE, color=plt.cm.YlGn(0.85), linewidth=2, label='mean HSE')\n",
    "ax.plot(np.arange(avg_HSN.size)/10000.-0.15,avg_HSN, color=plt.cm.Purples(0.99), linewidth=2, label='mean HSN')\n",
    "# ax.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04dbe628",
   "metadata": {},
   "source": [
    "# total average, in the form of average of averages, weighted by #members in each category\n",
    "minLen = min([avg_HSE.size, avg_HSN.size])\n",
    "avgAll = (avg_HSE[:minLen]*len(subtype_dict['HSE']) + avg_HSN[:minLen]*len(subtype_dict['HSN'])\n",
    "         ) /(len(subtype_dict['HSE']) + len(subtype_dict['HSN']))\n",
    "ax.plot(np.arange(avgAll.size)/10000-0.15, avgAll, color=[0.4,0.4,0.4], linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fbe356",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.axvline(0, linestyle='--', color='k')\n",
    "ax.set_xlim([-0.15,0.48])\n",
    "# f.set_size_inches(85/25.4, 50/25.4)\n",
    "add_scalebar(ax, matchx=False, sizex=0.1,  labelx='100 ms', matchy=False,\n",
    "             sizey=1.0, labely='1 mV', barwidth=3)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.savefig(os.path.join(DATAPATH, 'spontaneousSaccades', 'oldData',\n",
    "#                        'cwtSorted_oldData_spontSaccc_codedSubtype_perFly_AvgOnly_ephys.png'))\n",
    "# f.savefig(os.path.join(DATAPATH, 'spontaneousSaccades', 'oldData',\n",
    "#                        'cwtSorted_oldData_spontSaccc_codedSubtype_perFly_AvgOnly_ephys.svg'))\n",
    "\n",
    "# f.savefig(os.path.join(DATAPATH, 'spontaneousSaccades', 'oldData',\n",
    "#                        'cwtSorted_oldData_spontSaccc_codedSubtype_perFly_AvgOnly_incLegend_ephys.png'))\n",
    "# f.savefig(os.path.join(DATAPATH, 'spontaneousSaccades', 'oldData',\n",
    "#                        'cwtSorted_oldData_spontSaccc_codedSubtype_perFly_AvgOnly_incLegend_ephys.svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73caa73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot of mean +- sd\n",
    "\n",
    "f,ax=plt.subplots(1,1)\n",
    "\n",
    "plotcolors = {'HSE':plt.cm.YlGn(0.85), 'HSN':plt.cm.Purples(0.98), 'HSS': plt.cm.pink(0.2)}\n",
    "plotAlphas = [115/255, 77/255, 80/255]\n",
    "\n",
    "ax.plot(np.arange(avg_HSS.size)/10000.-0.15, avg_HSS, color=plotcolors['HSS'], linewidth=2)\n",
    "ax.fill_between(np.arange(avg_HSS.size)/10000.-0.15, avg_HSS+std_HSS, avg_HSS-std_HSS, \n",
    "                color=plotcolors['HSS'], alpha=plotAlphas[2])\n",
    "\n",
    "ax.plot(np.arange(avg_HSE.size)/10000.-0.15, avg_HSE, color=plotcolors['HSE'], linewidth=2)\n",
    "ax.fill_between(np.arange(avg_HSE.size)/10000.-0.15, avg_HSE+std_HSE, avg_HSE-std_HSE, \n",
    "                color=plotcolors['HSE'], alpha=plotAlphas[0])\n",
    "\n",
    "ax.plot(np.arange(avg_HSN.size)/10000.-0.15, avg_HSN, color=plotcolors['HSN'], linewidth=2)\n",
    "ax.fill_between(np.arange(avg_HSN.size)/10000.-0.15, avg_HSN+std_HSN, avg_HSN-std_HSN, \n",
    "                color=plotcolors['HSN'], alpha=plotAlphas[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c46cd",
   "metadata": {},
   "source": [
    "### WBA"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49751f27",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# simpler version or first look\n",
    "import plotly.express as px\n",
    "px.line(allSaccLeftMeanWBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a21d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,1, figsize=(7,5))\n",
    "\n",
    "plotlist = subtype_dict['HSE']\n",
    "\n",
    "plotcolors = plt.cm.YlGn(np.linspace(0.2,0.7,len(plotlist)))\n",
    "i=-1\n",
    "for k in plotlist:\n",
    "    #if k in behavioral_outliers:    continue\n",
    "    print(k)\n",
    "    data = allSaccLeftMeanWBA[k]\n",
    "    i+=1\n",
    "    ax.plot(np.arange(data.size)/10000.-0.15, data, color=plotcolors[i], linewidth=1., label=k)\n",
    "# ax.legend()\n",
    "# add average line\n",
    "minLen = min([allSaccLeftMeanWBA[fid].size for fid in plotlist])\n",
    "alldata = np.array([allSaccLeftMeanWBA[fid][:minLen] for fid in plotlist])\n",
    "avg_HSE = np.mean(alldata, axis=0)\n",
    "std_HSE = np.std(alldata, axis=0)/len(plotlist)\n",
    "# ax.plot(np.arange(avg_HSE.size)/10000.-0.15,avg_HSE, color=plt.cm.YlGn(0.85), linewidth=2)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9800b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotlist = subtype_dict['HSS']\n",
    "\n",
    "plotcolors = plt.cm.pink(np.linspace(0.3,0.4,len(plotlist)))\n",
    "i=-1\n",
    "for k in plotlist:\n",
    "    #if k in behavioral_outliers:    continue\n",
    "    print(k)\n",
    "    data = allSaccLeftMeanWBA[k]\n",
    "    i+=1\n",
    "    ax.plot(np.arange(data.size)/10000.-0.15, data, color=plotcolors[i], linewidth=1., label=k)\n",
    "# ax.legend()\n",
    "# add average line\n",
    "minLen = min([allSaccLeftMeanWBA[fid].size for fid in plotlist])\n",
    "alldata = np.array([allSaccLeftMeanWBA[fid][:minLen] for fid in plotlist])\n",
    "avg_HSS = np.mean(alldata, axis=0)\n",
    "std_HSS = np.std(alldata, axis=0)/len(plotlist)\n",
    "# ax.plot(np.arange(avg_HSS.size)/10000.-0.15,avg_HSS, color=plt.cm.Purples(0.98), linewidth=2)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d340f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotlist = subtype_dict['HSN']\n",
    "\n",
    "plotcolors = plt.cm.Purples(np.linspace(0.2,0.7,len(plotlist)))\n",
    "i=-1\n",
    "for k in plotlist:\n",
    "    #if k in behavioral_outliers:    continue\n",
    "    print(k)\n",
    "    data = allSaccLeftMeanWBA[k]\n",
    "    i+=1\n",
    "    ax.plot(np.arange(data.size)/10000.-0.15, data, color=plotcolors[i], linewidth=1., label=k)\n",
    "# ax.legend()\n",
    "# add average line\n",
    "minLen = min([allSaccLeftMeanWBA[fid].size for fid in plotlist])\n",
    "alldata = np.array([allSaccLeftMeanWBA[fid][:minLen] for fid in plotlist])\n",
    "avg_HSN = np.mean(alldata, axis=0)\n",
    "std_HSN = np.std(alldata, axis=0)/len(plotlist)\n",
    "# ax.plot(np.arange(avg_HSN.size)/10000.-0.15,avg_HSN, color=plt.cm.Purples(0.98), linewidth=2)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e70ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.plot(np.arange(avg_HSS.size)/10000.-0.15,avg_HSS, color=plt.cm.pink(0.2), linewidth=2, label='mean HSS')\n",
    "ax.plot(np.arange(avg_HSE.size)/10000.-0.15,avg_HSE, color=plt.cm.YlGn(0.85), linewidth=2, label='mean HSE')\n",
    "ax.plot(np.arange(avg_HSN.size)/10000.-0.15,avg_HSN, color=plt.cm.Purples(0.98), linewidth=2, label='mean HSN')\n",
    "# ax.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58ddc781",
   "metadata": {},
   "source": [
    "# total average, in the form of average of averages, weighted by #members in each category\n",
    "minLen = min([avg_HSE.size, avg_HSN.size])\n",
    "avgAll = (avg_HSE[:minLen]*len(subtype_dict['HSE']) + avg_HSN[:minLen]*len(subtype_dict['HSN'])\n",
    "         ) /(len(subtype_dict['HSE']) + len(subtype_dict['HSN']))\n",
    "ax.plot(np.arange(avgAll.size)/10000-0.15, avgAll, color=[0.4,0.4,0.4], linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4656473",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.axvline(0, linestyle='--', color='k')\n",
    "ax.set_xlim([-0.15,0.48])\n",
    "# f.set_size_inches(85/25.4, 50/25.4)\n",
    "add_scalebar(ax, matchx=False, sizex=0.1,  labelx='100 ms', matchy=False,\n",
    "             sizey=5.0, labely='5°', barwidth=3)\n",
    "#             sizey=10.0, labely='10°', barwidth=3)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b0aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.legend()\n",
    "# ax.get_legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ebfa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.savefig(os.path.join(DATAPATH, 'spontaneousSaccades', 'oldData',\n",
    "#                        'cwtSorted_oldData_spontSaccc_codedSubtype_perFly_AvgOnly_wingDiff.png'))\n",
    "# f.savefig(os.path.join(DATAPATH, 'spontaneousSaccades', 'oldData',\n",
    "#                        'cwtSorted_oldData_spontSaccc_codedSubtype_perFly_AvgOnly_wingDiff.svg'))\n",
    "\n",
    "# f.savefig(os.path.join(DATAPATH, 'spontaneousSaccades', 'oldData',\n",
    "#                        'cwtSorted_oldData_spontSaccc_codedSubtype_perFly_AvgOnly_incLegend_wingDiff.png'))\n",
    "# f.savefig(os.path.join(DATAPATH, 'spontaneousSaccades', 'oldData',\n",
    "#                        'cwtSorted_oldData_spontSaccc_codedSubtype_perFly_AvgOnly_incLegend_wingDiff.svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cc6a4a",
   "metadata": {},
   "source": [
    "## plot amplitude of behavior vs ephys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is modified form above, redo to put in dict structure\n",
    "# refer to previosu cell to avoid mistakes\n",
    "polAmp_dict={}\n",
    "saccAmp_dict={}\n",
    "\n",
    "plotlist = list(allSaccRightMeanEphys.keys())[:]\n",
    "\n",
    "for flyid in plotlist[:]:\n",
    "    #f,ax = plt.subplots(1,2)\n",
    "    \n",
    "    d=allSaccLeftMeanEphys[flyid]\n",
    "    #ax[0].plot(d)\n",
    "    d = ndimage.gaussian_filter1d(d, 60)\n",
    "    #ax[0].plot(d)\n",
    "    ix = int(np.argmax(signal.medfilt(np.abs(d[int(0.15*10000):int(0.35*10000)]), 101))+0.15*10000)\n",
    "    #ax[0].plot(ix, d[ix], 'ro')\n",
    "    polAmp_dict[flyid] = d[ix]\n",
    "\n",
    "    d=allSaccLeftMeanWBA[flyid]\n",
    "    #ax[1].plot(d)\n",
    "    #d = signal.medfilt(d, 201)\n",
    "    d = ndimage.gaussian_filter1d(d, 60)\n",
    "    #ax[1].plot(d)\n",
    "    ix = int(np.argmax(signal.medfilt(np.abs(d[int(0.15*10000):int(0.35*10000)]), 101))+0.15*10000)\n",
    "    #ax[1].plot(ix, d[ix], 'ro')\n",
    "    saccAmp_dict[flyid] = d[ix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa89f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simpler versionf or first look\n",
    "import plotly.express as px\n",
    "px.scatter(x=saccAmp_dict, y=polAmp_dict, text=list(saccAmp_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8029217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,1, figsize=(5,4))\n",
    "\n",
    "#HSE\n",
    "# plotlist = subtype_dict['HSE']\n",
    "# remove the problematic one(s)\n",
    "plotlist = [flyid for flyid in subtype_dict['HSE'] if not flyid in behavioral_outliers]\n",
    "plotcolors = plt.cm.YlGn(np.linspace(0.2,0.7,len(plotlist)))\n",
    "i=0\n",
    "print('HSE')\n",
    "for i,flyid in enumerate(plotlist):\n",
    "    if flyid in behavioral_outliers:\n",
    "        continue\n",
    "    print(flyid)\n",
    "    #ax.plot(np.arange(data.size)/10000.-0.15, data, color=plotcolors[i], linewidth=1.)\n",
    "    ax.scatter(saccAmp_dict[flyid], polAmp_dict[flyid], color=plotcolors[i], marker='o')\n",
    "    i+=1\n",
    "# calculate average dot\n",
    "# avg_HSE_wba = np.mean( [saccAmp_dict[flyid] for flyid in plotlist])\n",
    "# avg_HSE_ephys = np.mean( [polAmp_dict[flyid] for flyid in plotlist])\n",
    "# std_HSE_wba = np.std( [saccAmp_dict[flyid] for flyid in plotlist])\n",
    "# std_HSE_ephys = np.std( [polAmp_dict[flyid] for flyid in plotlist])\n",
    "# median and quartile\n",
    "avg_HSE_wba = np.median( [saccAmp_dict[flyid] for flyid in plotlist])\n",
    "avg_HSE_ephys = np.median( [polAmp_dict[flyid] for flyid in plotlist])\n",
    "quant_HSE_wba = np.quantile( [saccAmp_dict[flyid] for flyid in plotlist], [0.25,0.75])\n",
    "quant_HSE_ephys = np.quantile( [polAmp_dict[flyid] for flyid in plotlist], [0.25,0.75])\n",
    "\n",
    "# HSS\n",
    "# plotlist = subtype_dict['HSS']\n",
    "plotlist = [flyid for flyid in subtype_dict['HSS'] if not flyid in behavioral_outliers]\n",
    "plotcolors = plt.cm.pink(np.linspace(0.3,0.4,len(plotlist)))\n",
    "i=0\n",
    "print('HSS')\n",
    "for i,flyid in enumerate(plotlist):\n",
    "    if flyid in behavioral_outliers:\n",
    "        continue\n",
    "    print(flyid)\n",
    "    #ax.plot(np.arange(data.size)/10000.-0.15, data, color=plotcolors[i], linewidth=1.)\n",
    "    ax.plot(saccAmp_dict[flyid], polAmp_dict[flyid], color=plotcolors[i], marker='o')\n",
    "    i+=1\n",
    "# avg_HSS_wba = np.mean( [saccAmp_dict[flyid] for flyid in plotlist])\n",
    "# avg_HSS_ephys = np.mean( [polAmp_dict[flyid] for flyid in plotlist])\n",
    "# std_HSS_wba = np.std( [saccAmp_dict[flyid] for flyid in plotlist])\n",
    "# std_HSS_ephys = np.std( [polAmp_dict[flyid] for flyid in plotlist])\n",
    "# median and quartile\n",
    "avg_HSS_wba = np.median( [saccAmp_dict[flyid] for flyid in plotlist])\n",
    "avg_HSS_ephys = np.median( [polAmp_dict[flyid] for flyid in plotlist])\n",
    "quant_HSS_wba = np.quantile( [saccAmp_dict[flyid] for flyid in plotlist], [0.25,0.75])\n",
    "quant_HSS_ephys = np.quantile( [polAmp_dict[flyid] for flyid in plotlist], [0.25,0.75])\n",
    "\n",
    "# HSN\n",
    "# plotlist = subtype_dict['HSN']\n",
    "plotlist = [flyid for flyid in subtype_dict['HSN'] if not flyid in behavioral_outliers]\n",
    "plotcolors = plt.cm.Purples(np.linspace(0.2,0.7,len(plotlist)))\n",
    "i=0\n",
    "print('HSN')\n",
    "for i,flyid in enumerate(plotlist):\n",
    "    if flyid in behavioral_outliers:\n",
    "        continue\n",
    "    print(flyid)\n",
    "    #ax.plot(np.arange(data.size)/10000.-0.15, data, color=plotcolors[i], linewidth=1.)\n",
    "    ax.plot(saccAmp_dict[flyid], polAmp_dict[flyid], color=plotcolors[i], marker='o')\n",
    "    i+=1\n",
    "# avg_HSN_wba = np.mean( [saccAmp_dict[flyid] for flyid in plotlist])\n",
    "# avg_HSN_ephys = np.mean( [polAmp_dict[flyid] for flyid in plotlist])\n",
    "# std_HSN_wba = np.std( [saccAmp_dict[flyid] for flyid in plotlist])\n",
    "# std_HSN_ephys = np.std( [polAmp_dict[flyid] for flyid in plotlist])\n",
    "# # median and quartile\n",
    "avg_HSN_wba = np.median( [saccAmp_dict[flyid] for flyid in plotlist])\n",
    "avg_HSN_ephys = np.median( [polAmp_dict[flyid] for flyid in plotlist])\n",
    "quant_HSN_wba = np.quantile( [saccAmp_dict[flyid] for flyid in plotlist], [0.25,0.75])\n",
    "quant_HSN_ephys = np.quantile( [polAmp_dict[flyid] for flyid in plotlist], [0.25,0.75])\n",
    "\n",
    "\n",
    "ax.set_xlabel('mean saccade amplitude [°]')\n",
    "ax.set_ylabel('mean saccade-associated potential [mV]')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d3276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add average dots\n",
    "ax.plot(avg_HSE_wba, avg_HSE_ephys, color=plt.cm.YlGn(0.85),\n",
    "        marker='s',markersize=10,markeredgecolor='k',linewidth=2)\n",
    "ax.plot(avg_HSS_wba, avg_HSS_ephys, color=plt.cm.pink(0.2),\n",
    "        marker='s',markersize=10,markeredgecolor='k',linewidth=2)\n",
    "ax.plot(avg_HSN_wba, avg_HSN_ephys, color=plt.cm.Purples(0.98),\n",
    "        marker='s',markersize=10,markeredgecolor='k',linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55cdffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting mean +- sd\n",
    "ax.errorbar(avg_HSE_wba, avg_HSE_ephys, xerr=std_HSE_wba, yerr=std_HSE_ephys, color=plt.cm.YlGn(0.85),\n",
    "        marker='s',markersize=10,markeredgecolor='k',linewidth=2)\n",
    "ax.errorbar(avg_HSS_wba, avg_HSS_ephys, xerr=std_HSS_wba, yerr=std_HSS_ephys, color=plt.cm.pink(0.2),\n",
    "        marker='s',markersize=10,markeredgecolor='k',linewidth=2)\n",
    "ax.errorbar(avg_HSN_wba, avg_HSN_ephys, xerr=std_HSN_wba, yerr=std_HSN_ephys, color=plt.cm.Purples(0.98),\n",
    "        marker='s',markersize=10,markeredgecolor='k',linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot whiskers for percentiles\n",
    "\n",
    "ax.plot(quant_HSE_wba,[avg_HSE_ephys,avg_HSE_ephys], color=plt.cm.YlGn(0.85), marker=None,linewidth=2)\n",
    "ax.plot([avg_HSE_wba,avg_HSE_wba], quant_HSE_ephys, color=plt.cm.YlGn(0.85), marker=None,linewidth=2)\n",
    "\n",
    "ax.plot(quant_HSS_wba,[avg_HSS_ephys,avg_HSS_ephys], color=plt.cm.pink(0.2), marker=None,linewidth=2)\n",
    "ax.plot([avg_HSS_wba,avg_HSS_wba], quant_HSS_ephys, color=plt.cm.pink(0.2), marker=None,linewidth=2)\n",
    "\n",
    "ax.plot(quant_HSN_wba,[avg_HSN_ephys,avg_HSN_ephys], color=plt.cm.Purples(0.98), marker=None,linewidth=2)\n",
    "ax.plot([avg_HSN_wba,avg_HSN_wba], quant_HSN_ephys, color=plt.cm.Purples(0.98), marker=None,linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118c4e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.savefig(os.path.join(DATAPATH,'spontaneousSaccades','oldData',\n",
    "#                         'cwtSorted_oldData_spontanSacc_codedSubtype_wbaVSephys.png'))\n",
    "                         'cwtSorted_oldData_spontanSacc_codedSubtype_wbaVSephys_meanStd.png'))\n",
    "#                        'cwtSorted_oldData_spontanSacc_codedSubtype_wbaVSephys_medianQuartile.png'))\n",
    "f.savefig(os.path.join(DATAPATH,'spontaneousSaccades','oldData',\n",
    "#                         'cwtSorted_oldData_spontanSacc_codedSubtype_wbaVSephys.svg'))\n",
    "                         'cwtSorted_oldData_spontanSacc_codedSubtype_wbaVSephys_meanStd.svg'))\n",
    "#                        'cwtSorted_oldData_spontanSacc_codedSubtype_wbaVSephys_medianQuartile.svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a000a3a",
   "metadata": {},
   "source": [
    "## correlation wba <-> ephys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba4943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "saccAmp = [v for k,v in saccAmp_dict.items() if not k in behavioral_outliers]\n",
    "polAmp = [v for k,v in polAmp_dict.items() if not k in behavioral_outliers]\n",
    "\n",
    "print(f'Pearson R: {stats.pearsonr(saccAmp, polAmp)}')\n",
    "print(f'Spearman rho: {stats.spearmanr(saccAmp, polAmp)}')\n",
    "stats.linregress(saccAmp, polAmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6fdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r,p = stats.pearsonr(saccAmp, polAmp)\n",
    "ax.text(0.075, 0.8, f\"Pearson's r={r:.2f}\\np={p:.2f}\", transform=ax.transAxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43600bee",
   "metadata": {},
   "source": [
    "## correlation wba <-> ephys\n",
    "In a first approach, the obvious thing everyone thinks of is a linear correlation. However, that has the underlying assumption that the intercept can be anything, which is kind of nonsensical in this concept. After all, that would mean the by flying straight (<= L-R WBA=0 at intercept), there is a voltage response in either direction.\n",
    "\n",
    "Therefore we should do a linear reression of y = a*x with intercept fixed to 0.\n",
    "\n",
    "In summary, normal linear correlation seems to indicate good correlation, including statistical significance in the case of Pearson's r, but as explained, it's nonsense. When not allowing intercepts, the slope can not predict the observed polarizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2419cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saccAmp = [v for k,v in saccAmp_dict.items() if not k in behavioral_outliers]\n",
    "# polAmp = [v for k,v in polAmp_dict.items() if not k in behavioral_outliers]\n",
    "allflies = list(set(subtype_dict['HSE']) | set(subtype_dict['HSN']) | set(subtype_dict['HSS']))\n",
    "saccAmp = [saccAmp_dict[k] for k in allflies]\n",
    "polAmp = [polAmp_dict[k] for k in allflies]\n",
    "\n",
    "print(f'Pearson R: {stats.pearsonr(saccAmp, polAmp)}')\n",
    "print(f'Spearman rho: {stats.spearmanr(saccAmp, polAmp)}')\n",
    "stats.linregress(saccAmp, polAmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a35be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION WITH FIXED INTERCEPT TO 0\n",
    "# for forcing an intercept of a=0, we fit the equation y=b*x \n",
    "linreg = np.linalg.lstsq(np.array(saccAmp).reshape(-1,1), np.array(polAmp).reshape(-1,1))\n",
    "print(linreg)\n",
    "f,ax=plt.subplots(figsize=(5,3.5))\n",
    "ax.plot(saccAmp, polAmp, 'bo')\n",
    "ax.plot(saccAmp, saccAmp*linreg[0][0], 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28116356",
   "metadata": {},
   "source": [
    "What I think makes more sense is to compare the rank correlations for a model that is agnostic to the subtype (basically what has been calculated above), to a model which is agnostic to saccade amplitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1066b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "allflies = list(subtype_dict['HSN']) #use lists and extend to assert the order\n",
    "allflies.extend(list(subtype_dict['HSE']))\n",
    "allflies.extend(list(subtype_dict['HSS']))\n",
    "\n",
    "#set up dummy variable\n",
    "saccAmp = [0]*len(subtype_dict['HSN'])\n",
    "saccAmp.extend([1]*len(subtype_dict['HSE']))\n",
    "saccAmp.extend([2]*len(subtype_dict['HSS']))\n",
    "# now this one ist guaranteed to be in same order\n",
    "polAmp = [polAmp_dict[k] for k in allflies]\n",
    "\n",
    "print(f'Spearman rho: {stats.spearmanr(saccAmp, polAmp)}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7bbd13a",
   "metadata": {},
   "source": [
    "r,p = stats.pearsonr(saccAmp, polAmp)\n",
    "ax.text(0.075, 0.8, f\"Pearson's r={r:.2f} \\np={p:.2f}\", transform=ax.transAxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e067f30f",
   "metadata": {},
   "source": [
    "If a causal link exists between L-R WBA and Vm, then we should expect to see some of it within the subgroups as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130dc9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get correlation within each subtype\n",
    "## if figure is still active, we can add lines to it for subtype lin regressions\n",
    "toPlot = True\n",
    "if toPlot:\n",
    "    f=plt.gcf()\n",
    "    ax=f.get_axes()[0]\n",
    "\n",
    "subtypeFlies = list(subtype_dict['HSN'])\n",
    "saccAmp = np.array([saccAmp_dict[k] for k in subtypeFlies])\n",
    "polAmp = [polAmp_dict[k] for k in subtypeFlies]\n",
    "linreg = stats.linregress(saccAmp, polAmp)\n",
    "print('HSN \\n',linreg)\n",
    "if toPlot:\n",
    "    ax.plot(np.sort(saccAmp), np.sort(saccAmp)*linreg.slope+linreg.intercept, '--', color=plt.cm.Purples(0.6))\n",
    "\n",
    "subtypeFlies = list(subtype_dict['HSE'])\n",
    "saccAmp = np.array([saccAmp_dict[k] for k in subtypeFlies])\n",
    "polAmp = [polAmp_dict[k] for k in subtypeFlies]\n",
    "linreg = stats.linregress(saccAmp, polAmp)\n",
    "print('HSE \\n',linreg)\n",
    "if toPlot:\n",
    "    ax.plot(np.sort(saccAmp), np.sort(saccAmp)*linreg.slope+linreg.intercept, '--', color=plt.cm.YlGn(0.6))\n",
    "\n",
    "subtypeFlies = list(subtype_dict['HSS'])\n",
    "saccAmp = np.array([saccAmp_dict[k] for k in subtypeFlies])\n",
    "polAmp = [polAmp_dict[k] for k in subtypeFlies]\n",
    "linreg = stats.linregress(saccAmp, polAmp)\n",
    "print('HSS \\n',linreg)\n",
    "if toPlot:\n",
    "    ax.plot(np.sort(saccAmp), np.sort(saccAmp)*linreg.slope+linreg.intercept, '--', color=plt.cm.pink(0.2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60db2fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8fc5dc",
   "metadata": {},
   "source": [
    "## statistics on amplitudes\n",
    "Considering the sizes of the subpopulations and that they are apparently not normally distributed, it's better to use the Mann-Whitney-U (non-parametric) test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c2be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### ephys ######\n",
    "hse_vals = [polAmp_dict[flyid] for flyid in subtype_dict['HSE']]\n",
    "hsn_vals = [polAmp_dict[flyid] for flyid in subtype_dict['HSN']]\n",
    "hss_vals = [polAmp_dict[flyid] for flyid in subtype_dict['HSS']]\n",
    "\n",
    "print('HSN vs HSS', stats.mannwhitneyu(hsn_vals, hss_vals, method='exact'))\n",
    "print('HSN vs HSE', stats.mannwhitneyu(hsn_vals, hse_vals, method='exact'))\n",
    "print('HSS vs HSE', stats.mannwhitneyu(hse_vals, hss_vals, method='exact'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d397b063",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### WBA ######\n",
    "hse_vals = [saccAmp_dict[flyid] for flyid in subtype_dict['HSE']]\n",
    "hsn_vals = [saccAmp_dict[flyid] for flyid in subtype_dict['HSN']]\n",
    "hss_vals = [saccAmp_dict[flyid] for flyid in subtype_dict['HSS']]\n",
    "\n",
    "print('HSN vs HSS', stats.mannwhitneyu(hsn_vals, hss_vals, method='exact'))\n",
    "print('HSN vs HSE', stats.mannwhitneyu(hsn_vals, hse_vals, method='exact'))\n",
    "print('HSS vs HSE', stats.mannwhitneyu(hse_vals, hss_vals, method='exact'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aa3f78",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dacd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as psp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d9310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f=go.Figure()\n",
    "f= psp.make_subplots(rows=2,cols=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67270759",
   "metadata": {},
   "outputs": [],
   "source": [
    "allLeft = []\n",
    "allRight = []\n",
    "\n",
    "#f,ax=plt.subplots(2,2, figsize=(9,8))\n",
    "f = psp.make_subplots(rows=2,cols=2)\n",
    "# for el in range(11,12):\n",
    "for el in range(len(anlyz)):\n",
    "    #if not el in pureYvals.keys():   continue\n",
    "    if not isinstance(saccDataCWT[el]['peakTime'], (list,np.ndarray)):\n",
    "        saccDataCWT[el] = {k:np.array([v]) for k,v in saccDataCWT[el].items()}\n",
    "    if len(saccDataCWT[el]['peakTime'])==0:  continue\n",
    "    v = saccDataCWT[el]\n",
    "    #actualOnset= (v['onset']*anlyz.meanFramePeriod*anlyz.samplingRate).astype(int)\n",
    "    actualOnset= (v['CWTpeaks']*0.02*anlyz.samplingRate).astype(int)\n",
    "    #pdb.set_trace()\n",
    "    saccAmp = v['saccAmp']\n",
    "    rightSacc = []\n",
    "    leftSacc=[]\n",
    "    #chunk = int(0.5*anlyz.samplingRate)\n",
    "    chunk = int(0.5*anlyz.samplingRate)\n",
    "    preOnset = int(0.25*anlyz.samplingRate)\n",
    "    ephys = anlyz.segments[el].data[0]#ephys data\n",
    "    wba = anlyz.segments[el].data[-1]*180/np.pi\n",
    "    #f,ax=plt.subplots(2,1, figsize=(10,4))\n",
    "    #ax[0].plot(wba)\n",
    "    #ax[1].plot(ephys)\n",
    "    #for o,a in zip(actualOnset, saccAmp):\n",
    "    for j in range(saccAmp.size):\n",
    "        o=actualOnset[j]\n",
    "        a=saccAmp[j]\n",
    "        p= (v['peakTime']*0.02*anlyz.samplingRate).astype(int)[j]\n",
    "        #ax[0].plot(o,wba[o], 'r>')\n",
    "        #ax[0].plot(p, wba[p],'k^' )\n",
    "        #xval = np.arange(o-preOnset, o+chunk)/anlyz.samplingRate\n",
    "        if a<0:\n",
    "            leftSacc.append(ephys[o-preOnset:o+chunk])\n",
    "            #leftSacc.append(ephys[o-preOnset:p+chunk])\n",
    "            d = wba[o-preOnset:o+chunk]\n",
    "            xval = np.arange(-preOnset, d.size-preOnset)/anlyz.samplingRate\n",
    "            #ax[0,1].plot(xval, d-np.mean(d[:int(0.25*anlyz.samplingRate)]))#have to adjust this for PWC-fitted data\n",
    "            f.add_trace(go.Scatter(x=xval, y=d-np.mean(d[:int(0.25*anlyz.samplingRate)])), row=1,col=2)\n",
    "            d = ephys[o-preOnset:o+chunk]\n",
    "            #ax[1,1].plot(xval, d-np.mean(d[:int(0.25*anlyz.samplingRate)]))\n",
    "            f.add_trace(go.Scatter(x=xval, y=d-np.mean(d[:int(0.25*anlyz.samplingRate)])), row=2,col=2)\n",
    "        else:\n",
    "            rightSacc.append(ephys[o-preOnset:o+chunk])\n",
    "            #rightSacc.append(ephys[o-preOnset:p+chunk])\n",
    "            d = wba[o-preOnset:o+chunk]\n",
    "            xval = np.arange(-preOnset,d.size-preOnset)/anlyz.samplingRate\n",
    "            #ax[0,0].plot(xval, d-np.mean(d[:int(0.25*anlyz.samplingRate)]))#have to adjust this for PWC-fitted data\n",
    "            f.add_trace(go.Scatter(x=xval, y=d-np.mean(d[:int(0.25*anlyz.samplingRate)])), row=1,col=1)\n",
    "            d = ephys[o-preOnset:o+chunk]\n",
    "            #ax[1,0].plot(xval, d-np.mean(d[:int(0.25*anlyz.samplingRate)]))\n",
    "            f.add_trace(go.Scatter(x=xval, y=d-np.mean(d[:int(0.25*anlyz.samplingRate)])), row=2,col=1)\n",
    "    allLeft.extend(leftSacc)\n",
    "    allRight.extend(rightSacc)\n",
    "# ax[0,0].set_title('saccades to right')\n",
    "# ax[0,1].set_title('saccades to left')\n",
    "# ax[0,0].set_ylabel('L-R WBA')\n",
    "# ax[1,0].set_ylabel('U [mV]')\n",
    "# ax[1,0].set_xlabel('time [s]')\n",
    "# ax[1,1].set_xlabel('time [s]')\n",
    "#ax[0,0].set_ylim([-0.15, 0.51])\n",
    "#ax[0,1].set_ylim([-0.51, 0.15])\n",
    "#ax[1,0].set_ylim([-4, 4])\n",
    "#ax[1,1].set_ylim([-4, 4])\n",
    "#ax[0,0].set_ylim([-0.21, 0.45])\n",
    "# [a.set_xlim([-0.25,0.5]) for a in ax.flatten()]\n",
    "\n",
    "# [a.axvline(0., color='k', linestyle=':') for a in ax.flatten()]    \n",
    "\n",
    "# post-hoc adding a mean\n",
    "i=0\n",
    "\"\"\"for a in ax.flatten():\n",
    "    ally=[c.get_data()[1] for c in a.get_children() if isinstance(c, plt.Line2D)]\n",
    "    ally=[y for y in ally if len(y)>100]\n",
    "    #print([len(y) for y in ally])\n",
    "    try:\n",
    "        minLen = min([len(y) for y in ally])\n",
    "    #     padLen = int(np.median([len(y) for y in ally])) # !why median!?\n",
    "        padLen = int(np.max([len(y) for y in ally]))\n",
    "        #print(minLen)\n",
    "        ally = [np.pad(y,(0,padLen-len(y)), mode='constant', constant_values=np.nan) for y in ally]\n",
    "        #ally = np.array([y[:minLen] for y in ally])\n",
    "        ally = np.array(ally)\n",
    "        a.plot(np.arange(-preOnset,ally.shape[1]-preOnset)/anlyz.samplingRate, np.nanmean(ally,axis=0), 'k-')\n",
    "\n",
    "    #     mu = np.nanmean(ally,axis=0)\n",
    "    #     a.plot(np.arange(-preOnset,ally.shape[1]-preOnset)/anlyz.samplingRate, (mu-np.nanmean(mu))/np.nanstd(mu), 'r-')\n",
    "\n",
    "        if i==0:\n",
    "            anlyz.saccRightMeanWBA = np.nanmean(ally,axis=0)\n",
    "        elif i==1:\n",
    "            anlyz.saccLeftMeanWBA = np.nanmean(ally,axis=0)\n",
    "        elif i==2:\n",
    "            anlyz.saccRightMeanEphys = np.nanmean(ally,axis=0)\n",
    "        elif i==3:\n",
    "            anlyz.saccLeftMeanEphys = np.nanmean(ally,axis=0)\n",
    "        i+=1\n",
    "    except ValueError: #if min() arg is an empty sequence\n",
    "        ally = np.zeros(0)\n",
    "        \n",
    "    #a.set_xlim([-preOnset,padLen/anlyz.samplingRate])\n",
    "    a.text(0.05, 0.95, f'n={ally.shape[0]}', transform=a.transAxes, fontsize=14,\n",
    "    verticalalignment='top')\n",
    "\n",
    "f.suptitle(anlyz.flyID[0]+'_'+anlyz.metadata[0]['HS_type'])\n",
    "plt.tight_layout()\"\"\"\n",
    "f.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "277.467px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
